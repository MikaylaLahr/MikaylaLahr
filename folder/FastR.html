<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Fast Robots</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Simple line icons-->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.5.5/css/simple-line-icons.min.css" rel="stylesheet" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- About-->
        <section class="content-section bg-light" id="about">
            <div class="container px-4 px-lg-5 text-center">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-lg-10">
                        <h2>Fast Robots</h2>
                        <h3><a href="https://www.engineering.cornell.edu/faculty-directory/jonathan-jaramillo">Prof. Jonathan Jaramillo</a></h3>
                        <h3>January - May 2024</h3>
                        <div class="row gx-4 gx-lg-5 justify-content-center">
                            <div class="col-lg-10" style="text-align: left;">
                                <p class="lead mb-5">Grade: A+</p>
                                <h2>Objectives</h2>
                                <ul>
                                    <strong>Lab 1</strong>
                                    <strong>Lab 2</strong>
                                    <strong>Lab 3</strong>
                                    <strong>Lab 4</strong>
                                    <strong>Lab 5</strong>
                                    <strong>Lab 6</strong>
                                    <strong>Lab 7</strong>
                                    <strong>Lab 8</strong>
                                    <strong>Lab 9</strong>
                                    <li>Mapping</li>
                                    <li>Orientation Control</li>
                                    <li>ToF Distance Sensor Readings</li>
                                    <strong>Lab 10</strong>
                                    <li>Localization Simulation</li>
                                    <li>Write functions to compute the control, odometry motion, prediction step, 
                                        sensor model, and update step. 
                                    </li>
                                    <strong>Lab 11</strong>
                                    <li>Localization on Robot</li>
                                    <li>Compare the Belief and Ground Truth</li>
                                    <li>Orientation Control</li>
                                    <li>ToF Distance Sensor Readings</li>
                                </ul>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>  
<!__ Lab 11: Localization On The Robot ______________________________________________________________________________>
                                <h2 class="mb-2.5">Lab 11: Localization On The Robot</h2> 
                                <p>The objective of the lab is to implement the Bayes Filter on the real robot. Due to noise when 
                                    the robot is turning, only the update step of the filter will be be used during a 360 degree
                                    scan.
                                </p>
<!-- Lab 11 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Simulation Localization</h3> <!------------------------------------------------->
                    <p>The first lab task was to run the provided Bayes Filter simulation. The lab11_sim.ipynb file was 
                        downloaded and a screenshot of the final result is below as well as a 
                        <a href="https://docs.google.com/document/d/120ek_jK8Y9CPmBWIn1pVJgnnejH-On_O0puMCUnvyTg/edit">link to the data</a>
                        from the simulation. 
                        The red line is odometry, the green line is the ground truth, and the blue line is the 
                        belief. The map is a three dimensional grid (x, y, theta). The dimension of x is 12, the dimension 
                        of y is 9, and the dimension of theta is 18. 
                    </p>
                    <img width="500" height="auto" src="Lab11/SimulationGraph.png"></img>
                    <p></p>
                    <h3 class="mb-0">Run Robot Localization</h3> <!-------------------------------------------------->
                    <p>The results of rotating the robot at the four marked positions are below. The four marked positions 
                        include: (-3ft, -2ft, 0deg), (0ft, 3ft, 0deg), (5ft, -3ft, 0deg), and (5ft, 3ft, 0deg). The robot 
                        was run around 4-5 times at each point. Two runs for each point are shown below. The ground truth 
                        is plotted as the green dot and is the location where the robot rotated. The belief of the robots 
                        position is plotted as the blue dot. The ToF sensor used to collect distance readings was located 
                        on the front of the robot, between the left and right sides of the robot. When the robot was at 
                        zero degrees the ToF sensor was pointed towards the rightside wall. The robot turned counterclockwise
                        in the same manner as in Lab 9. The Jupyter Notebook code to plot each Ground Truth value is below:
                    </p>
                     <img width="300" height="auto" src="Lab11/Points.png"></img>
                    <h4 class="mb-0">Bottom Left Point (-3ft, -2ft, 0deg):</h4> <!----------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/BL1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/BL1_GroundTruth.png"></img>
                        <p>For the bottom left point, the Belief was at the same point as the Ground Truth. 
                            As can be seen in the below polar plot, the robot sensed the surrounding walls very well. 
                            I reran the rotation several (5) times and this point had the most rotations result in the
                            Belief being the same as the Ground Truth. This is a result of the several walls surrounding 
                            the robot when rotating. The next run shows one of the runs where the Belief was different 
                            then the Ground Truth.
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/BL1_PolarPlot.png"></img>
                        
                    <h4 class="mb-0">Top Middle Point (0ft, 3ft, 0deg):</h4> <!-------------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/TM1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/TM1_GroundTruth.png"></img>
                        <p>For the top middle point, the Belief was at the same point as the Ground Truth. 
                            I was surprised to achieve this result as there are not a lot of nearby walls to use for 
                            localization and I suspected that this would be the hardest point to localize. As can be 
                            seen in the below polar plot, the robot was able to sense the surrounding walls and obstacles. 
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/TM1_PolarPlot.png"></img>
                    
                    <h4 class="mb-0">Bottom Right Point (5ft, -3ft, 0deg):</h4> <!----------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/BR1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/BR1_GroundTruth.png"></img>
                        <p>For the bottom right point, the Belief was at the same point as the Ground Truth. 
                            As can be seen in the below polar plot, the robot rotated well and was able to sense the 
                            surrounding walls and obstacles. 
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/BR1_PolarPlot.png"></img>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>   
<!__ Lab 10: Grid Localization Using Bayes Filter (Simulation) ______________________________________________________>
                    <h2 class="mb-2.5">Lab 10: Grid Localization Using Bayes Filter</h2> 
                    <p>The objective of the lab is to use the Bayes Filter to implement grid localization. 
                    </p>
                    <p>Before starting the lab, I familiarized myself with how the Bayes Filter worked and how to 
                        implement the Bayes filter for the robot in simulation. Localization and of the robot determines 
                        where the robot is in the enviroment. There is a predition and update step 
                        of the Bayes filter. The prediction step uses the control input and accounts for the noise 
                        from the actuator to predict the new location of the robot. After the prediction step, the 
                        robot rotates 360 degrees gathering distance measurement data. The update step uses the gathered 
                        distance measurements to determine the real location of the robot. The Bayes Filter from Lecture 16 
                        is below: 
                    </p>
                    <img width="350" height="auto" src="Lab10/BayesFilterEq.png"></img>
                    <p>The dimensions of each grid cell in the x, y, and theta axes are 0.3048 meters, 0.3048 meters, 
                        and 20 degrees. The grid is (12,9,18) in three dimensions, totalling 1944 cells. The discritized 
                        grid map is:
                        <li>(-1.6764,+1.9812) meters or (-5.5,6.5) feet in the x-axis</li>
                        <li>(-1.3716,+1.3716) meters or (-4.5,4.5) feet in the y-axis</li>
                        <li>(-180,+180) degrees in the theta-axis</li>
                    </p>
<!-- Lab 10 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Implementation</h3> <!---------------------------------------------------------->
                    <p>Five functions were written to run the Bayes Filter as outlined below.</p>
                    <h4 class="mb-0">Compute Control</h4> <!--------------------------------------------------------->
                    <p>The control information from the robot is used to find the first rotation, translation, and 
                        second rotation. The inputs include the current position (cur_pose) and the previous position 
                        (prev_pose). The return values include the first rotation (delta_rot_1), the translation 
                        (delta_trans), and the second rotation (delta_rot_2). The Lecture 17 slide with the equations
                        to find the odometry model parameters is below: 
                    </p>
                    <img width="450" height="auto" src="Lab10/ComputeControlEq.png"></img>
                    <p>The above equations were modeled in Jupyter Notebook. The code is found below. First the 
                        differences in the y and x positions are found (y_comp and x_comp). Then the normalized rotations 
                        are calculated as well as the translation. 
                    </p>
                    <img width="700" height="auto" src="Lab10/compute_control.png"></img>
                    <h4 class="mb-0">Odometry Motion Model</h4> <!--------------------------------------------------->
                    <p>The odometry motion model finds the probability that the robot performed the motion that was 
                        calculated in the previous step. The inputs include the current position (cur_pose), previous
                        position (prev_pose), and odometry control values (u) for rotation 1, translation, and rotation 
                        2. The return value is the probablility ( p(x'|x,u) ) that the robot achieved the current 
                        position (x'), knowing the previous position (x) and odometry control values (u). A Gaussian 
                        distribution is used. Each motion is assumed to be an independent event. The probability of
                        each event is then multiplied together to find the probability of the entire motion. The Jupyter 
                        Notebook code is found below.
                    </p>
                    <img width="700" height="auto" src="Lab10/odom_motion_model.png"></img>
                    <h4 class="mb-0">Prediction Step</h4> <!--------------------------------------------------------->
                    <p>The prediction step of the Bayes Filter as implemented in Jupyter Notebook uses six for loops 
                        to calculate the probability that the robot moved to the next grid cell. The inputs include the
                        current odometry position (cur_odom) and the previous odometry position (prev_odom).
                        The if statement in the pediction step code is 
                        used to speed up the filter. If the belief is less than 0.0001 than the three remaining 
                        for loops will not be entered as the robot is unlikely to be in the remaining grid cells. If 
                        the belief is greater than or equal to 0.0001 the probability will be calculated. The code for 
                        calculating the probability is below.
                    </p>
                    <img width="700" height="auto" src="Lab10/prediction_step.png"></img>
                    <h4 class="mb-0">Sensor Model</h4> <!------------------------------------------------------------>
                    <p>The sensor model function finds the probability of the sensor measurements, modelled as a 
                        Gaussian distribution. The length of the array is 18 as the robot measures 18 different distance 
                        reading values. The input includes the 1D observation array (obs) for the robot position. The
                        function returns a 1D array of the same length including the likelihood of each event (prob_array).
                        The Jupyter Notebook code is below.
                    </p>
                    <img width="700" height="auto" src="Lab10/sensor_model.png"></img>
                    <h4 class="mb-0">Update Step</h4> <!------------------------------------------------------------->
                    <p>The update step calculates the belief of the robot for each grid cell. The probability ( p(z|x) )
                        is multiplied by the predicted belief (loc.bel_bar) to find the belief (loc.bel). The belief is 
                        then normalized. The code can be found below.
                    </p>
                    <img width="700" height="auto" src="Lab10/update_step.png"></img>
                    <p></p>
                    <h3 class="mb-0">Simulation Results</h3> <!------------------------------------------------------>
                    <p>Two runs of the Bayes Filter simulation are found below. The green line is the ground truth, 
                        the blue line is the belief, and the red line is the odometry measurements. The lighter grid 
                        cells represent a higher belief. For both runs the ground truth and the belief are close 
                        together. Both runs were 15 iterations.
                    </p>
                    <h4 class="mb-0">Run 1 Results</h4> <!----------------------------------------------------------->
                    <iframe width="550" height="400" src="https://youtube.com/embed/VLH8bvA__mk"></iframe>
                    <img width="400" height="auto" src="Lab10/Run1Graph.png"></img>
                    <p></p>
                    <h4 class="mb-0">Run 2 Results</h4> <!----------------------------------------------------------->
                    <iframe width="550" height="400" src="https://youtube.com/embed/4hKQfm8IZOE"></iframe>
                    <img width="400" height="auto" src="Lab10/Run2Graph.png"></img>
                    <p></p>

<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>   
<!__ Lab 9: Mapping __________________________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 9: Mapping</h2> 
                    <p>The objective of the lab is to use yaw angle measurements from the IMU sensor and distance 
                        readings from the ToF sensor to map a room. The map will be used during localization and navigation 
                        tasks. Map quality depends on how many readings are obtained and the separation between each 
                        reading during each rotation. 
                    </p>
<!-- Lab 9 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Orientation Control</h3> <!---------------------------------------------------->
                        <p>Orientation Control was chosen as the PID controller for the lab. The PID controller 
                            results in the robot performing on-axis turns in 10 degree increments. The ToF sensor used
                            to collect distance readings is located on the front of the robot between the left and right 
                            motor sides. 
                        </p>
                    <h4 class="mb-0">Code For Orientation Control</h4> <!------------------------------------------->
                        <p>First 
                            the distance is found using the readings from the ToF sensors, then the yaw angle is found 
                            using the IMU sensor. The PID controller value is used in the RIGHT_MOTOR and LEFT_MOTOR 
                            functions to determine the motor input value. If the error between the current yaw angle 
                            and the setAngle (10 degrees) is less than -0.5 degrees or greater than 0.5 degrees then 
                            the RIGHT_MOTOR and LEFT_MOTOR functions will run. The "stop" variable is set to be 360 
                            divided by the "setAngle" variable, which is equal to 10. The results in stop = 36 
                            increments. If variable "end" is less than "stop" the robot will stop for one second. 
                            I stopped the robot for one second to ensure that the ToF sensor is pointed towards a 
                            fixed point in space. 
                            The total yaw angle is recorded by adding the current total to the current yaw value (yaw_arr). 
                            The yaw value is then set to zero for the next iteration of the while loop. Resetting the 
                            yaw value for the IMU sensor reduces gyroscope drift.
                            If the "end" 
                            value is greater than or equal to the "stop" value the robot will stop and no new data will 
                            be recorded into the yaw_arr array. 
                        </p>
                        <p>The Jupyter Notebook command and Arduino case to start the PID orientation controller are 
                            below. From testing I found that using a Kp value of 1, Ki value of 0 and Kd value of 0 
                            worked to move the robot in 10 degree increments. Each time the PID orientation controller
                            starts, yaw[0], yaw_arr[0], and the total are set to zero. In addition the stop variable 
                            is found. Graphs showing the changing yaw angles to confirm the PID controller works are 
                            found in the next section.
                        </p>
                    <h4 class="mb-0">Video and PID Control Graphs</h4> <!------------------------------------------->
                        <p>Two videos of the robot turning are below. The robot roughly turns on axis. In the second 
                            video, the robot overshoots at some increments but is able to correct itself to the correct 
                            angle. The robot stays inside of one floor tile for the duration of the turn.
                        </p>
                        <p>Turn 1:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/yQFxBwK8bnc"></iframe>
                        <p>Turn 2:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/bgJ58W0-rNg"></iframe>
                        <p>I plotted the values of yaw for each 10 degree increment. As can be seen in the below graph
                            the PID controller works as expected. After yaw increases to 10 degrees, the value of yaw 
                            is reset to zero and then again increases to 10 degrees for the next increment. The slight 
                            variation in the final yaw value is due to the -0.5 to 0.5 degree tolerance. 
                        </p>
                        <img width="500" height="auto" src="Lab9/BR_Yaw_Test.png"></img>
                        <p>The total angle values were found by adding each final yaw value (between 9.5 and 10.5 
                            degrees) to a summation of all previous yaw values. The values and graph for an example 
                            turn are found below. 
                        </p>
                        <img width="900" height="auto" src="Lab9/BR_Yaw_0_360.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw_Angles.png"></img>
                        <p>In the above graph as the turn progresses the angles remain accurate as the summation 
                            calculation is correct to the yaw reading of the robot, but the angles slightly vary from 
                            the ideal 360 degree turn values. To address this I also performed half turns from 0 to 180
                            degrees, and 180 to 360 degrees. The angle values and corresponding graphs can be found 
                            below:
                        </p>
                        <img width="600" height="auto" src="Lab9/BR_Yaw_0_180.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw180.png"></img>
                    <hr>
                        <img width="600" height="auto" src="Lab9/BR_Yaw_180_360.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw360.png"></img>
                        <p>The half turns in the above graphs are more in line with the ideal angles. When testing the 
                            robot in the lab, I collected angle data for full turns and half turns.
                        </p>
                    <h3 class="mb-0">Read Out Distances</h3> <!----------------------------------------------------->
                    <h4 class="mb-0">Execute Turn At Each Marked Position</h4> <!----------------------------------->
                        <p>The robot was placed at each of the following points: (-3,-2), (0,0), (0,3), (5,3), and (5,-3).
                            The image below shows each of the points in the lab. 
                        </p>
                        <img width="500" height="auto" src="Lab9/LabPoints.png"></img>
                        <p>The distance was measured using the front ToF sensor between the left and right motor sides. 
                            The data collected was sent over Bluetooth to Jupyter Notebook to be plotted. The robot 
                            was started in the same orietation, facing the right side wall of the lab, for each full 
                            turn and each half turn starting at zero degrees. For half turns starting at 180 degrees, 
                            the robot started facing the left side wall. The range of potential angle values for each 
                            ideal 10 degree rotation is 9.5 degrees to 10.5 degrees. There is a -/+ 0.5 degree range 
                            for the robot to stop and collect a distance measurement. Due to the range of degree values,
                            it can not be assumed that the robot will stop at exactly 10 degrees every time it rotates.
                            To address this issue, I did not assume that the robot stopped at exactly 
                            10 degrees for every rotation and instead recorded a "total" variable value that added the 
                            angle that the robot stopped at to the previously recorded stopping angle. This allowed me 
                            to plot the distance measurements with the correct recorded angles.
                        </p>
                        
                        <h3 class="mb-0">Polar And Global Frame Plots</h3> <!----------------------------------->
                        <p>This section details the second attempt at collecting data at each point. The data collected 
                            for each point in the lab is plotted in the below graphs. I readjusted the ToF sensor to point 
                            more upwards before collecting the following data. This adjustement removed noise from inside 
                            the map region. The data collected for each point in the lab is plotted in the below graphs.
                        </p>
                        <h5 class="mb-0">Bottom Left: Point (-3,-2)</h5> <!---------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarBL.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalBL.png"></img>
                        <h5 class="mb-0">Middle: Point (0,0)</h5> <!----------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarM.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalM.png"></img>
                        <h5 class="mb-0">Top Middle: Point (0,3)</h5> <!------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarTM.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalTM.png"></img>
                        <h5 class="mb-0">Top Right: Point (5,3)</h5> <!-------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarTR.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalTR.png"></img>
                        <h5 class="mb-0">Bottom Right: Point (5,-3)</h5> <!---------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarBR.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalBR.png"></img>
                                
                    <h4 class="mb-0">Merge And Plot Readings</h4> <!---------------------------------------->
                    <p>I plotted the distances in the global frame using a
                        transformation matrix to convert the frame of the robot used to collect the sensor measurements 
                        into the global frame. 
                    </p>
                    <p>Below are the distance readings in the global frame.</p>
                        <img width="500" height="auto" src="Lab9/Test2/AllGlobal2.png"></img>
                        <img width="200" height="auto" src="Lab9/Test2/Legend2.png"></img>
                    <h3 class="mb-0">Convert To Line Based Map</h3> <!---------------------------------------------->
                    <p>After plotting the above graph I plotted lines representing the walls of the map. The plot showing 
                        the line based map is below as well as the code used to plot each line. 
                    </p>
                        <img width="500" height="auto" src="Lab9/Test2/AllGlobal2Lines.png"></img>

                    <p></p>

<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>      
<!__ Lab 8 Extra Credit ___________________________________________________________________________________________________>
                    <h2 class="mb-0">Lab 8 Extra Credit: Implement Kalman Filter On Robot</h2> <!-------------------------->
                        <p>I implemented the Kalman Filter on the robot for extra credit. To complete this task I needed 
                            to install the Basic Linear Algebra Library on Arduino IDE.
                            To implement the Kalman 
                            Filter I first created two new cases in Arduino IDE: KF_DATA and SEND_KF_DATA. These cases 
                            are similar to the cases used to start/stop the PID controller and send PID controller data,
                            but I added array's for Kalman Filter time (KF_Time) and the Kalman filter distance 
                            (mu_Store). I added a delay in KF_CASE before running the while loop. When I was testing the
                            filter I was repeatedly starting with distances of 0mm. The delay after the ToF distance 
                            sensor starts ranging reduces the amount of 0mm distance readings when starting the filter. 
                        </p>
                        <p>After creating the cases for the Kalman Filter, I then started working on writing a Kalman 
                            Filter function in Arduino IDE. The final initialization of the matrices is below:
                        </p>
                         <img width="350" height="auto" src="Lab8EC/KF_Matrix.png"></img>
                        <p></p>
                    <h3 class="mb-0">Step 1: Kalman Filter Function</h3> <!------------------------------------------>
                        <p>The first function that I wrote for the lab was the function Kalman_Filter() that is called 
                            from inside the larger while loop. To perform this task, I referenced the Jupyter Notebook 
                            code I wrote for Lab 7 and modified the code for the Arduino IDE syntax. The function can 
                            be seen below:
                        </p>
                        <img width="500" height="auto" src="Lab8EC/KF_Function.png"></img>
                        <p>The Ad (discritized A) and Bd (discritized B) matrices are updated each time the Kalman Filter
                            is called to use the most recent dt value. I printed the dt (time difference) values to the 
                            Serial Monitor and observed that the dt values slightly varied each time the while loop ran. 
                            The variable dt is how fast the while loop runs and ranged from around 10ms to 25ms. After
                            finding Ad and Bd, mu_p and sigma_p are calculated. The if statement of the filter is the 
                            update step and runs when there is a new TOF sensor reading (newTOF == 1). The else statement
                            is the prediction step and assigns the mu_p value to mu and the sigma_p value to sigma. 
                            During both steps the first value for mu is stored in the array mu_Store. 
                        </p>
                                <p></p>
                    <h3 class="mb-0">Step 2: Kalman Filter Update Step On Robot</h3> <!----------------------------->
                    <h4 class="mb-0">Using ToF Readings For PID Loop</h4> <!---------------------------------------->
                        <p>I started working on the calling the Kalman Filter from inside the while loop by modifying 
                            the script for my PID controller. After finding the ToF sensor readings and running the PID 
                            controller within the larger while loop, I added an if statement inside the loop to call the
                            Kalman Filter to find the update values. The if statement runs if dStart == 1 and 
                            if the currect distance value is not equal to the previous distance value. The variable dStart
                            is set to 1 if the new ToF sensor distance is not the same value as the previous sensor distance.
                            If the current distance reading is the same as the previous distance reading dStart is set to 0. 
                            The if statement to call the update of the Kalman Filter is below:
                        </p>
                        <img width="500" height="auto" src="Lab8EC/P1Update.png"></img>
                        <p>The PID controller was run using values of Kp = 0.05, Ki = 0.0000008, and Kd = 0.5. A test run 
                            showing the plotted ToF data and the Kalman Filter updates is shown below. It can be seen that
                            the Kalman Filter updates align with each new reading of the ToF sensor. 
                        </p>
                        <img width="400" height="auto" src="Lab8EC/K1/K1Full.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K1/K1P1.png"></img>
                        <p>The video below shows the Serial Monitor outputs for the ToF distance readings and the update 
                            values:
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/Yn4fIvG2Z2M"></iframe>
                        <p>After successfully implemeting the update step of the Kalman Filter, I started working on 
                            the prediction step. 
                        </p>
                    <h3 class="mb-0">Step 3: Kalman Filter Update and Prediction Steps On Robot</h3> <!------------->
                    <h4 class="mb-0">Using ToF Readings and Predicted Distances For PID Loop</h4> <!---------------->
                        <p>I started the next step by removing the PID control from the larger while loop and creating 
                            a separate function called PID_Function(). A separate function for PID control allowed for 
                            me to more easily switch the order to performing the Kalman Filter before the PID controller.
                            Previously the PID controller used the ToF sensor measurements every time the loop ran. In 
                            the new iteration of the code, if the update step of the filter runs, the distance measurement 
                            used will be the ToF sensor reading. If the prediction step runs the most current value of mu
                            will be used in the PID controller. The if and else statements for the update and prediction steps
                            are shown below. The variable dis is the 
                            distance measurement used in the PID controller. The u value for the Kalman Filter was calculated 
                            the same as in Jupyter Notebook using the PID control values. 
                        </p>
                        <img width="500" height="auto" src="Lab8EC/P2UpdateAndPredict.png"></img>
                        <p>The PID_Function is below:</p>
                        <img width="500" height="auto" src="Lab8EC/PID_Function.png"></img>
                    <p>Throughout testing, I determined that a good value to multuiply the B matrix with is 8. The Kalman Filter 
                        updates align with each new reading of the ToF sensor and the prediction values follow the trend of the data. 
                        Below are two sets of graphs showing data that was recorded after the B matrix was multiplied by 8. Needing to 
                        multiply the B matrix by 8 indicates that the value found for m during the Jupyter Lab simulation does not work 
                        as well when implementing the Kalman Filter on the robot. 
                    </p>
                    <p>Test 1:</p>
                        <img width="500" height="auto" src="Lab8EC/K6/K6Full.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K6/K6P1.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K6/K6P2.png"></img>
                    <hr>
                    <p>Test 2:</p>
                        <img width="500" height="auto" src="Lab8EC/K7/K7Full.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K7/K7P1.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K7/K7P2.png"></img>
                    <hr>
                        <p>The video below shows the Serial Monitor outputs for the ToF distance readings and the update 
                            values:
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/PYxf5fFNzd0"></iframe>
                                <p></p>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
                                
        <!-- Scroll to Top Button-->
        <a class="scroll-to-top rounded" href="#page-top"><i class="fas fa-angle-up"></i></a>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
