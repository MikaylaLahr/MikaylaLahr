<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Fast Robots</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Simple line icons-->
        <link href="https://cdnjs.cloudflare.com/ajax/libs/simple-line-icons/2.5.5/css/simple-line-icons.min.css" rel="stylesheet" />
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,300italic,400italic,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- About-->
        <section class="content-section bg-light" id="about">
            <div class="container px-4 px-lg-5 text-center">
                <div class="row gx-4 gx-lg-5 justify-content-center">
                    <div class="col-lg-10">
                        <h2>Fast Robots</h2>
                        <h3><a href="https://www.engineering.cornell.edu/faculty-directory/jonathan-jaramillo">Prof. Jonathan Jaramillo</a></h3>
                        <h3>January - May 2024</h3>
                        <div class="row gx-4 gx-lg-5 justify-content-center">
                            <div class="col-lg-10" style="text-align: left;">
                                <p class="lead mb-5">Grade: A+</p>
                                <h2>Objectives</h2>
                                <ul>
                                    <strong>Lab 1</strong>
                                    <strong>Lab 2</strong>
                                    <strong>Lab 3</strong>
                                    <strong>Lab 4</strong>
                                    <strong>Lab 5</strong>
                                    <strong>Lab 6</strong>
                                    <strong>Lab 7</strong>
                                    <li>Kalman Filter Using Collected ToF Distance Sensor Data</li>
                                    <strong>Lab 8</strong>
                                    <li>Implement the Kalman Filter on the Robot</li>
                                    <strong>Lab 9</strong>
                                    <li>Mapping</li>
                                    <li>Orientation Control</li>
                                    <li>ToF Distance Sensor Readings</li>
                                    <strong>Lab 10</strong>
                                    <li>Localization Simulation</li>
                                    <li>Write functions to compute the control, odometry motion, prediction step, 
                                        sensor model, and update step. 
                                    </li>
                                    <strong>Lab 11</strong>
                                    <li>Localization on Robot</li>
                                    <li>Compare the Belief and Ground Truth</li>
                                    <li>Orientation Control</li>
                                    <li>ToF Distance Sensor Readings</li>
                                </ul>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>  
<!__ Lab 11: Localization On The Robot ______________________________________________________________________________>
                                <h2 class="mb-2.5">Lab 11: Localization On The Robot</h2> 
                                <p>The objective of the lab is to implement the Bayes Filter on the real robot. Due to noise when 
                                    the robot is turning, only the update step of the filter will be be used during a 360 degree
                                    scan.
                                </p>
<!-- Lab 11 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Simulation Localization</h3> <!------------------------------------------------->
                    <p>The first lab task was to run the provided Bayes Filter simulation. The lab11_sim.ipynb file was 
                        downloaded and a screenshot of the final result is below as well as a 
                        <a href="https://docs.google.com/document/d/120ek_jK8Y9CPmBWIn1pVJgnnejH-On_O0puMCUnvyTg/edit">link to the data</a>
                        from the simulation. 
                        The red line is odometry, the green line is the ground truth, and the blue line is the 
                        belief. The map is a three dimensional grid (x, y, theta). The dimension of x is 12, the dimension 
                        of y is 9, and the dimension of theta is 18. 
                    </p>
                    <img width="500" height="auto" src="Lab11/SimulationGraph.png"></img>
                    <p></p>
                    <h3 class="mb-0">Run Robot Localization</h3> <!-------------------------------------------------->
                    <p>The results of rotating the robot at the four marked positions are below. The four marked positions 
                        include: (-3ft, -2ft, 0deg), (0ft, 3ft, 0deg), (5ft, -3ft, 0deg), and (5ft, 3ft, 0deg). The robot 
                        was run around 4-5 times at each point. Two runs for each point are shown below. The ground truth 
                        is plotted as the green dot and is the location where the robot rotated. The belief of the robots 
                        position is plotted as the blue dot. The ToF sensor used to collect distance readings was located 
                        on the front of the robot, between the left and right sides of the robot. When the robot was at 
                        zero degrees the ToF sensor was pointed towards the rightside wall. The robot turned counterclockwise
                        in the same manner as in Lab 9. The Jupyter Notebook code to plot each Ground Truth value is below:
                    </p>
                     <img width="300" height="auto" src="Lab11/Points.png"></img>
                    <h4 class="mb-0">Bottom Left Point (-3ft, -2ft, 0deg):</h4> <!----------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/BL1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/BL1_GroundTruth.png"></img>
                        <p>For the bottom left point, the Belief was at the same point as the Ground Truth. 
                            As can be seen in the below polar plot, the robot sensed the surrounding walls very well. 
                            I reran the rotation several (5) times and this point had the most rotations result in the
                            Belief being the same as the Ground Truth. This is a result of the several walls surrounding 
                            the robot when rotating. The next run shows one of the runs where the Belief was different 
                            then the Ground Truth.
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/BL1_PolarPlot.png"></img>
                        
                    <h4 class="mb-0">Top Middle Point (0ft, 3ft, 0deg):</h4> <!-------------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/TM1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/TM1_GroundTruth.png"></img>
                        <p>For the top middle point, the Belief was at the same point as the Ground Truth. 
                            I was surprised to achieve this result as there are not a lot of nearby walls to use for 
                            localization and I suspected that this would be the hardest point to localize. As can be 
                            seen in the below polar plot, the robot was able to sense the surrounding walls and obstacles. 
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/TM1_PolarPlot.png"></img>
                    
                    <h4 class="mb-0">Bottom Right Point (5ft, -3ft, 0deg):</h4> <!----------------------------------->
                        <p>Graphs for the Belief (blue) and the Ground Truth (green):</p>
                        <img width="400" height="auto" src="Lab11/BR1_Belief.png"></img>
                        <img width="400" height="auto" src="Lab11/BR1_GroundTruth.png"></img>
                        <p>For the bottom right point, the Belief was at the same point as the Ground Truth. 
                            As can be seen in the below polar plot, the robot rotated well and was able to sense the 
                            surrounding walls and obstacles. 
                        </p>
                    <hr>
                        <p>Polar Plot and recorded data below:</p>
                        <img width="300" height="auto" src="Lab11/BR1_PolarPlot.png"></img>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>   
<!__ Lab 10: Grid Localization Using Bayes Filter (Simulation) ______________________________________________________>
                    <h2 class="mb-2.5">Lab 10: Grid Localization Using Bayes Filter</h2> 
                    <p>The objective of the lab is to use the Bayes Filter to implement grid localization. 
                    </p>
                    <p>Before starting the lab, I familiarized myself with how the Bayes Filter worked and how to 
                        implement the Bayes filter for the robot in simulation. Localization and of the robot determines 
                        where the robot is in the enviroment. There is a predition and update step 
                        of the Bayes filter. The prediction step uses the control input and accounts for the noise 
                        from the actuator to predict the new location of the robot. After the prediction step, the 
                        robot rotates 360 degrees gathering distance measurement data. The update step uses the gathered 
                        distance measurements to determine the real location of the robot. The Bayes Filter from Lecture 16 
                        is below: 
                    </p>
                    <img width="350" height="auto" src="Lab10/BayesFilterEq.png"></img>
                    <p>The dimensions of each grid cell in the x, y, and theta axes are 0.3048 meters, 0.3048 meters, 
                        and 20 degrees. The grid is (12,9,18) in three dimensions, totalling 1944 cells. The discritized 
                        grid map is:
                        <li>(-1.6764,+1.9812) meters or (-5.5,6.5) feet in the x-axis</li>
                        <li>(-1.3716,+1.3716) meters or (-4.5,4.5) feet in the y-axis</li>
                        <li>(-180,+180) degrees in the theta-axis</li>
                    </p>
<!-- Lab 10 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Implementation</h3> <!---------------------------------------------------------->
                    <p>Five functions were written to run the Bayes Filter as outlined below.</p>
                    <h4 class="mb-0">Compute Control</h4> <!--------------------------------------------------------->
                    <p>The control information from the robot is used to find the first rotation, translation, and 
                        second rotation. The inputs include the current position (cur_pose) and the previous position 
                        (prev_pose). The return values include the first rotation (delta_rot_1), the translation 
                        (delta_trans), and the second rotation (delta_rot_2). The Lecture 17 slide with the equations
                        to find the odometry model parameters is below: 
                    </p>
                    <img width="450" height="auto" src="Lab10/ComputeControlEq.png"></img>
                    <p>The above equations were modeled in Jupyter Notebook. The code is found below. First the 
                        differences in the y and x positions are found (y_comp and x_comp). Then the normalized rotations 
                        are calculated as well as the translation. 
                    </p>
                    <img width="700" height="auto" src="Lab10/compute_control.png"></img>
                    <h4 class="mb-0">Odometry Motion Model</h4> <!--------------------------------------------------->
                    <p>The odometry motion model finds the probability that the robot performed the motion that was 
                        calculated in the previous step. The inputs include the current position (cur_pose), previous
                        position (prev_pose), and odometry control values (u) for rotation 1, translation, and rotation 
                        2. The return value is the probablility ( p(x'|x,u) ) that the robot achieved the current 
                        position (x'), knowing the previous position (x) and odometry control values (u). A Gaussian 
                        distribution is used. Each motion is assumed to be an independent event. The probability of
                        each event is then multiplied together to find the probability of the entire motion. The Jupyter 
                        Notebook code is found below.
                    </p>
                    <img width="700" height="auto" src="Lab10/odom_motion_model.png"></img>
                    <h4 class="mb-0">Prediction Step</h4> <!--------------------------------------------------------->
                    <p>The prediction step of the Bayes Filter as implemented in Jupyter Notebook uses six for loops 
                        to calculate the probability that the robot moved to the next grid cell. The inputs include the
                        current odometry position (cur_odom) and the previous odometry position (prev_odom).
                        The if statement in the pediction step code is 
                        used to speed up the filter. If the belief is less than 0.0001 than the three remaining 
                        for loops will not be entered as the robot is unlikely to be in the remaining grid cells. If 
                        the belief is greater than or equal to 0.0001 the probability will be calculated. The code for 
                        calculating the probability is below.
                    </p>
                    <img width="700" height="auto" src="Lab10/prediction_step.png"></img>
                    <h4 class="mb-0">Sensor Model</h4> <!------------------------------------------------------------>
                    <p>The sensor model function finds the probability of the sensor measurements, modelled as a 
                        Gaussian distribution. The length of the array is 18 as the robot measures 18 different distance 
                        reading values. The input includes the 1D observation array (obs) for the robot position. The
                        function returns a 1D array of the same length including the likelihood of each event (prob_array).
                        The Jupyter Notebook code is below.
                    </p>
                    <img width="700" height="auto" src="Lab10/sensor_model.png"></img>
                    <h4 class="mb-0">Update Step</h4> <!------------------------------------------------------------->
                    <p>The update step calculates the belief of the robot for each grid cell. The probability ( p(z|x) )
                        is multiplied by the predicted belief (loc.bel_bar) to find the belief (loc.bel). The belief is 
                        then normalized. The code can be found below.
                    </p>
                    <img width="700" height="auto" src="Lab10/update_step.png"></img>
                    <p></p>
                    <h3 class="mb-0">Simulation Results</h3> <!------------------------------------------------------>
                    <p>Two runs of the Bayes Filter simulation are found below. The green line is the ground truth, 
                        the blue line is the belief, and the red line is the odometry measurements. The lighter grid 
                        cells represent a higher belief. For both runs the ground truth and the belief are close 
                        together. Both runs were 15 iterations.
                    </p>
                    <h4 class="mb-0">Run 1 Results</h4> <!----------------------------------------------------------->
                    <iframe width="550" height="400" src="https://youtube.com/embed/VLH8bvA__mk"></iframe>
                    <img width="400" height="auto" src="Lab10/Run1Graph.png"></img>
                    <p></p>
                    <h4 class="mb-0">Run 2 Results</h4> <!----------------------------------------------------------->
                    <iframe width="550" height="400" src="https://youtube.com/embed/4hKQfm8IZOE"></iframe>
                    <img width="400" height="auto" src="Lab10/Run2Graph.png"></img>
                    <p></p>

<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>
<!____________________________________________________________________________________________________________________>   
<!__ Lab 9: Mapping __________________________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 9: Mapping</h2> 
                    <p>The objective of the lab is to use yaw angle measurements from the IMU sensor and distance 
                        readings from the ToF sensor to map a room. The map will be used during localization and navigation 
                        tasks. Map quality depends on how many readings are obtained and the separation between each 
                        reading during each rotation. 
                    </p>
<!-- Lab 9 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Orientation Control</h3> <!---------------------------------------------------->
                        <p>Orientation Control was chosen as the PID controller for the lab. The PID controller 
                            results in the robot performing on-axis turns in 10 degree increments. The ToF sensor used
                            to collect distance readings is located on the front of the robot between the left and right 
                            motor sides. 
                        </p>
                    <h4 class="mb-0">Code For Orientation Control</h4> <!------------------------------------------->
                        <p>First 
                            the distance is found using the readings from the ToF sensors, then the yaw angle is found 
                            using the IMU sensor. The PID controller value is used in the RIGHT_MOTOR and LEFT_MOTOR 
                            functions to determine the motor input value. If the error between the current yaw angle 
                            and the setAngle (10 degrees) is less than -0.5 degrees or greater than 0.5 degrees then 
                            the RIGHT_MOTOR and LEFT_MOTOR functions will run. The "stop" variable is set to be 360 
                            divided by the "setAngle" variable, which is equal to 10. The results in stop = 36 
                            increments. If variable "end" is less than "stop" the robot will stop for one second. 
                            I stopped the robot for one second to ensure that the ToF sensor is pointed towards a 
                            fixed point in space. 
                            The total yaw angle is recorded by adding the current total to the current yaw value (yaw_arr). 
                            The yaw value is then set to zero for the next iteration of the while loop. Resetting the 
                            yaw value for the IMU sensor reduces gyroscope drift.
                            If the "end" 
                            value is greater than or equal to the "stop" value the robot will stop and no new data will 
                            be recorded into the yaw_arr array. 
                        </p>
                        <p>The Jupyter Notebook command and Arduino case to start the PID orientation controller are 
                            below. From testing I found that using a Kp value of 1, Ki value of 0 and Kd value of 0 
                            worked to move the robot in 10 degree increments. Each time the PID orientation controller
                            starts, yaw[0], yaw_arr[0], and the total are set to zero. In addition the stop variable 
                            is found. Graphs showing the changing yaw angles to confirm the PID controller works are 
                            found in the next section.
                        </p>
                    <h4 class="mb-0">Video and PID Control Graphs</h4> <!------------------------------------------->
                        <p>Two videos of the robot turning are below. The robot roughly turns on axis. In the second 
                            video, the robot overshoots at some increments but is able to correct itself to the correct 
                            angle. The robot stays inside of one floor tile for the duration of the turn.
                        </p>
                        <p>Turn 1:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/yQFxBwK8bnc"></iframe>
                        <p>Turn 2:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/bgJ58W0-rNg"></iframe>
                        <p>I plotted the values of yaw for each 10 degree increment. As can be seen in the below graph
                            the PID controller works as expected. After yaw increases to 10 degrees, the value of yaw 
                            is reset to zero and then again increases to 10 degrees for the next increment. The slight 
                            variation in the final yaw value is due to the -0.5 to 0.5 degree tolerance. 
                        </p>
                        <img width="500" height="auto" src="Lab9/BR_Yaw_Test.png"></img>
                        <p>The total angle values were found by adding each final yaw value (between 9.5 and 10.5 
                            degrees) to a summation of all previous yaw values. The values and graph for an example 
                            turn are found below. 
                        </p>
                        <img width="900" height="auto" src="Lab9/BR_Yaw_0_360.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw_Angles.png"></img>
                        <p>In the above graph as the turn progresses the angles remain accurate as the summation 
                            calculation is correct to the yaw reading of the robot, but the angles slightly vary from 
                            the ideal 360 degree turn values. To address this I also performed half turns from 0 to 180
                            degrees, and 180 to 360 degrees. The angle values and corresponding graphs can be found 
                            below:
                        </p>
                        <img width="600" height="auto" src="Lab9/BR_Yaw_0_180.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw180.png"></img>
                    <hr>
                        <img width="600" height="auto" src="Lab9/BR_Yaw_180_360.png"></img>
                        <img width="500" height="auto" src="Lab9/BR_Yaw360.png"></img>
                        <p>The half turns in the above graphs are more in line with the ideal angles. When testing the 
                            robot in the lab, I collected angle data for full turns and half turns.
                        </p>
                    <h3 class="mb-0">Read Out Distances</h3> <!----------------------------------------------------->
                    <h4 class="mb-0">Execute Turn At Each Marked Position</h4> <!----------------------------------->
                        <p>The robot was placed at each of the following points: (-3,-2), (0,0), (0,3), (5,3), and (5,-3).
                            The image below shows each of the points in the lab. 
                        </p>
                        <img width="500" height="auto" src="Lab9/LabPoints.png"></img>
                        <p>The distance was measured using the front ToF sensor between the left and right motor sides. 
                            The data collected was sent over Bluetooth to Jupyter Notebook to be plotted. The robot 
                            was started in the same orietation, facing the right side wall of the lab, for each full 
                            turn and each half turn starting at zero degrees. For half turns starting at 180 degrees, 
                            the robot started facing the left side wall. The range of potential angle values for each 
                            ideal 10 degree rotation is 9.5 degrees to 10.5 degrees. There is a -/+ 0.5 degree range 
                            for the robot to stop and collect a distance measurement. Due to the range of degree values,
                            it can not be assumed that the robot will stop at exactly 10 degrees every time it rotates.
                            To address this issue, I did not assume that the robot stopped at exactly 
                            10 degrees for every rotation and instead recorded a "total" variable value that added the 
                            angle that the robot stopped at to the previously recorded stopping angle. This allowed me 
                            to plot the distance measurements with the correct recorded angles.
                        </p>
                        
                        <h3 class="mb-0">Polar And Global Frame Plots</h3> <!----------------------------------->
                        <p>This section details the second attempt at collecting data at each point. The data collected 
                            for each point in the lab is plotted in the below graphs. I readjusted the ToF sensor to point 
                            more upwards before collecting the following data. This adjustement removed noise from inside 
                            the map region. The data collected for each point in the lab is plotted in the below graphs.
                        </p>
                        <h5 class="mb-0">Bottom Left: Point (-3,-2)</h5> <!---------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarBL.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalBL.png"></img>
                        <h5 class="mb-0">Middle: Point (0,0)</h5> <!----------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarM.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalM.png"></img>
                        <h5 class="mb-0">Top Middle: Point (0,3)</h5> <!------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarTM.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalTM.png"></img>
                        <h5 class="mb-0">Top Right: Point (5,3)</h5> <!-------------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarTR.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalTR.png"></img>
                        <h5 class="mb-0">Bottom Right: Point (5,-3)</h5> <!---------------------------------------->
                            <img width="300" height="auto" src="Lab9/Test2/2PolarBR.png"></img>
                            <img width="400" height="auto" src="Lab9/Test2/2GlobalBR.png"></img>
                                
                    <h4 class="mb-0">Merge And Plot Readings</h4> <!---------------------------------------->
                    <p>I plotted the distances in the global frame using a
                        transformation matrix to convert the frame of the robot used to collect the sensor measurements 
                        into the global frame. 
                    </p>
                    <p>Below are the distance readings in the global frame.</p>
                        <img width="500" height="auto" src="Lab9/Test2/AllGlobal2.png"></img>
                        <img width="200" height="auto" src="Lab9/Test2/Legend2.png"></img>
                    <h3 class="mb-0">Convert To Line Based Map</h3> <!---------------------------------------------->
                    <p>After plotting the above graph I plotted lines representing the walls of the map. The plot showing 
                        the line based map is below as well as the code used to plot each line. 
                    </p>
                        <img width="500" height="auto" src="Lab9/Test2/AllGlobal2Lines.png"></img>

                    <p></p>

<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>      
<!__ Lab 8 Extra Credit ___________________________________________________________________________________________________>
                    <h2 class="mb-0">Lab 8 Extra Credit: Implement Kalman Filter On Robot</h2> <!-------------------------->
                        <p>I implemented the Kalman Filter on the robot for extra credit. To complete this task I needed 
                            to install the Basic Linear Algebra Library on Arduino IDE.
                            To implement the Kalman 
                            Filter I first created two new cases in Arduino IDE: KF_DATA and SEND_KF_DATA. These cases 
                            are similar to the cases used to start/stop the PID controller and send PID controller data,
                            but I added array's for Kalman Filter time (KF_Time) and the Kalman filter distance 
                            (mu_Store). I added a delay in KF_CASE before running the while loop. When I was testing the
                            filter I was repeatedly starting with distances of 0mm. The delay after the ToF distance 
                            sensor starts ranging reduces the amount of 0mm distance readings when starting the filter. 
                        </p>
                        <p>After creating the cases for the Kalman Filter, I then started working on writing a Kalman 
                            Filter function in Arduino IDE. The final initialization of the matrices is below:
                        </p>
                         <img width="350" height="auto" src="Lab8EC/KF_Matrix.png"></img>
                        <p></p>
                    <h3 class="mb-0">Step 1: Kalman Filter Function</h3> <!------------------------------------------>
                        <p>The first function that I wrote for the lab was the function Kalman_Filter() that is called 
                            from inside the larger while loop. To perform this task, I referenced the Jupyter Notebook 
                            code I wrote for Lab 7 and modified the code for the Arduino IDE syntax. The function can 
                            be seen below:
                        </p>
                        <img width="500" height="auto" src="Lab8EC/KF_Function.png"></img>
                        <p>The Ad (discritized A) and Bd (discritized B) matrices are updated each time the Kalman Filter
                            is called to use the most recent dt value. I printed the dt (time difference) values to the 
                            Serial Monitor and observed that the dt values slightly varied each time the while loop ran. 
                            The variable dt is how fast the while loop runs and ranged from around 10ms to 25ms. After
                            finding Ad and Bd, mu_p and sigma_p are calculated. The if statement of the filter is the 
                            update step and runs when there is a new TOF sensor reading (newTOF == 1). The else statement
                            is the prediction step and assigns the mu_p value to mu and the sigma_p value to sigma. 
                            During both steps the first value for mu is stored in the array mu_Store. 
                        </p>
                                <p></p>
                    <h3 class="mb-0">Step 2: Kalman Filter Update Step On Robot</h3> <!----------------------------->
                    <h4 class="mb-0">Using ToF Readings For PID Loop</h4> <!---------------------------------------->
                        <p>I started working on the calling the Kalman Filter from inside the while loop by modifying 
                            the script for my PID controller. After finding the ToF sensor readings and running the PID 
                            controller within the larger while loop, I added an if statement inside the loop to call the
                            Kalman Filter to find the update values. The if statement runs if dStart == 1 and 
                            if the currect distance value is not equal to the previous distance value. The variable dStart
                            is set to 1 if the new ToF sensor distance is not the same value as the previous sensor distance.
                            If the current distance reading is the same as the previous distance reading dStart is set to 0. 
                            The if statement to call the update of the Kalman Filter is below:
                        </p>
                        <img width="500" height="auto" src="Lab8EC/P1Update.png"></img>
                        <p>The PID controller was run using values of Kp = 0.05, Ki = 0.0000008, and Kd = 0.5. A test run 
                            showing the plotted ToF data and the Kalman Filter updates is shown below. It can be seen that
                            the Kalman Filter updates align with each new reading of the ToF sensor. 
                        </p>
                        <img width="400" height="auto" src="Lab8EC/K1/K1Full.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K1/K1P1.png"></img>
                        <p>The video below shows the Serial Monitor outputs for the ToF distance readings and the update 
                            values:
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/Yn4fIvG2Z2M"></iframe>
                        <p>After successfully implemeting the update step of the Kalman Filter, I started working on 
                            the prediction step. 
                        </p>
                    <h3 class="mb-0">Step 3: Kalman Filter Update and Prediction Steps On Robot</h3> <!------------->
                    <h4 class="mb-0">Using ToF Readings and Predicted Distances For PID Loop</h4> <!---------------->
                        <p>I started the next step by removing the PID control from the larger while loop and creating 
                            a separate function called PID_Function(). A separate function for PID control allowed for 
                            me to more easily switch the order to performing the Kalman Filter before the PID controller.
                            Previously the PID controller used the ToF sensor measurements every time the loop ran. In 
                            the new iteration of the code, if the update step of the filter runs, the distance measurement 
                            used will be the ToF sensor reading. If the prediction step runs the most current value of mu
                            will be used in the PID controller. The if and else statements for the update and prediction steps
                            are shown below. The variable dis is the 
                            distance measurement used in the PID controller. The u value for the Kalman Filter was calculated 
                            the same as in Jupyter Notebook using the PID control values. 
                        </p>
                        <img width="500" height="auto" src="Lab8EC/P2UpdateAndPredict.png"></img>
                        <p>The PID_Function is below:</p>
                        <img width="500" height="auto" src="Lab8EC/PID_Function.png"></img>
                    <p>Throughout testing, I determined that a good value to multuiply the B matrix with is 8. The Kalman Filter 
                        updates align with each new reading of the ToF sensor and the prediction values follow the trend of the data. 
                        Below are two sets of graphs showing data that was recorded after the B matrix was multiplied by 8. Needing to 
                        multiply the B matrix by 8 indicates that the value found for m during the Jupyter Lab simulation does not work 
                        as well when implementing the Kalman Filter on the robot. 
                    </p>
                        <img width="400" height="auto" src="Lab8EC/K6/K6Full.png"></img> <p></p>
                        <img width="400" height="auto" src="Lab8EC/K6/K6P1.png"></img>
                        <img width="400" height="auto" src="Lab8EC/K6/K6P2.png"></img>
                    <hr>
                        <p>The video below shows the Serial Monitor outputs for the ToF distance readings and the update 
                            values:
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/PYxf5fFNzd0"></iframe>
                                <p></p>

<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>          
<!__ Lab 7: Kalman Filter ___________________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 7: Kalman Filter</h2>
                    <p>The objective of the lab is to gain experience implementing Kalman Filters. Labs 5-8 detail PID, 
                        sensor fusion, and stunts. This lab is focused on using a Kalman Filter to predict ToF sensor 
                        distance readings of the robot. The code from Lab 5: Linear PID Control will be used to gather
                        data, having the robot speed towards a wall and stop 1ft in front of the wall. A Kalman Filter
                        will be implemented in Jupyter Notebook using the collected data.
                    </p>
                    <h3 class="mb-0">Kalman Filter Explaination</h3> <!---------------------------------------------->
                    <p>A Kalman Filter uses a state space model to predict the location (distance from the wall) of 
                        the robot. The location of the robot is predicted and then updated using the ToF sensor reading.
                        The prediction and update steps from Lecture 13 are below:
                    </p>
                        <img width="350" height="auto" src="Lab7/PredictAndUpdate.png"></img>
                    <p>The state space model is used to find the A, B, and C matrices. The state space model for the 
                        robot from Lecture 13 is below. The model uses drag (d) and momentum (m) to form the A and B 
                        matrices. The two states are the location of the robot and the robots velocity.
                    </p>
                        <img width="200" height="auto" src="Lab7/StateSpace.png"></img>
                    <p>The first part of the lab is estimating the drag and momentum of the robot using steady state
                        velocity. One difference from the lecture slides is that for this lab the C matrix will be 
                        C = [1 0]
                    </p>
<!-- Lab 7 Lab Tasks ----------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">Estimate Drag and Momentum</h3> <!--------------------------------------------->
                    <p>The drag and momentum for the A and B matrices are estimated using a step response. The robot 
                        drives towards a wall at a constant input motor speed. The ToF sensor distance measurements 
                        and motor input values are recorded. Velocity is calculated and plotted along with the 90% rise
                        time. The data is used to determine the drag and momentum and build the state space model of the
                        system.
                    </p>
                    <p>The equations for drag and momentum are below:</p>
                         <img width="300" height="auto" src="Lab7/d_m_Matrices.png"></img>
                    <p>The step response u(t) was chosen to be a PWM value of 165. The step response is 64.71% of the 
                        maximum u PWM value of 255. I choose a PWM value of 165 as it was the fastest the robot could
                        move while still moving straight for a distance of around 6.5ft. I collected time, ToF distance 
                        readings, and used these readings to calculated the velocity.
                        To ensure that the step time 
                        was long enough for the robot to reach steady state, I ran the test multiple times. 
                    </p>
                        <img width="500" height="auto" src="Lab7/DM_Trial2.png"></img>
                    <p>From the above graph it can be seen that the steady state velocity is just over -2000mm/s, or 
                        more specifically -2031.91mm/s. Using a 90% rise time a line on the above graph was plotted 
                        showing 90% of the settling value. This value is (-2031.91mm/s * 0.9) = -1828.72mm/s. 
                        The 90% rise time can be determined from the graph as 1.32s. The steady state velocity is 
                        used to find the drag. The drag and rise time are used to find the momentum. The calculations 
                        to find drag (d) and momentum (m) are found below:
                    </p>
                         <img width="350" height="auto" src="Lab7/d_m_Calc.png"></img>
                    <p>Calculated: d = 0.0004921 and m = 0.0002821</p>
                    <p>Drag is inversly proportional to the speed of the robot and affects how much the system reacts to 
                        changes in u. Momentum relates to how much effort is needed to change the velocity. Higher 
                        momentum signifies that more effort needs to be exerted to change the velocity, whereas lower
                        momentum signifies that less effort needs to be exerted to change the velocity.
                    </p>
                    <p>The collected data array's from all trials were stored in Jupyter Notebook in case the data 
                        needed to be used in the future. The code to find the steady state velocity can be found here:
                        <a href="https://drive.google.com/file/d/1RMI90XZbYXhiNoc-Q6CnFWPwt4ZWEpcg/preview">Max Speed Code</a>
                    </p>
                    <h3 class="mb-0">Initialize Kalman Filter: Python</h3> <!--------------------------------------->
                    <h4 class="mb-0">Compute the A and B Matrices</h4> <!------------------------------------------->
                    <p>The A and B matrices were computed using the calculated values for d and m. From the state space 
                        equation the A and B matrices are:
                    </p>
                        <img width="300" height="auto" src="Lab7/DefineAB.png"></img>
                    <p>The A and B matrices were calculated as seen below:</p>
                        <img width="300" height="auto" src="Lab7/ABCalculation.png"></img>
                    <p>The discritized A and B matrices are calculated using the following method from Lecture 13:</p>
                        <img width="300" height="auto" src="Lab7/LectureAdBd.png"></img>
                    <p>Initially, the sampling time for the discritized Ad and Bd matrices was calculated by subtracting the first 
                        time stamp from the second time stamp. After looking closely at the data, the sampling time 
                        between each time step was slightly different. I changed the calculation for Delta_T to be an average
                        of all of the sampling time differences. The Delta_T was recalculated for each new dataset that I graphed.
                        Below is an example calculation of the discritized Ad and Bd matrices, using the found sampling time of 0.008s
                        for the PID loop in Lab 5. This dt is how fast the PID control loop is running. 
                    </p>
                        <img width="400" height="auto" src="Lab7/newAdBdCalculation.png"></img>
                    <p>The provided code in the lab report as well as my method of finding Delta_T was used to implement the 
                        calculation in Python as seen below:
                    </p>
                        <img width="400" height="auto" src="Lab7/AB_dt_AdBd_Calc.png"></img>
                    <p>There are two different time array's. One method of finding Delta_T uses the time array of the PID 
                        controller (array: time). This method was for the 5000 level task, resulting in a faster frequecy than 
                        the frequency found for the ToF readings (array: timeF). In between ToF readings the prediction step 
                        of the Kalman Filter estimates the state of the robot.
                    </p>
                    <p>The previously used code that does not calculate the average sampling time to find Ad and Bd can be found here: 
                        <a href="https://drive.google.com/file/d/1qQZAX09YhGPeyrxSfEZhoPeK9F1tjAGP/preview">Previous Ad Bd Code</a>
                    </p>
                    <h4 class="mb-0">Identify the C Matrix and Initialize the State Vector</h4> <!------------------>
                    <p>The C matrix for the state space is C = [1 0]. C is an m x n matrix where m is the number of 
                        states measured (1, ToF sensor distance) and n is the dimensions of the state space (2, ToF 
                        distance and velocity). The initial state is the first distance measurement and a velocity of 0
                        as the robot is not yet moving. The positive distance from the wall was measured.
                    </p>
                    <p>Below is the Python code to establish the C matrix and initialize the state vector for the ToF 
                        distance measurements.
                    </p>
                        <img width="550" height="auto" src="Lab7/C_x_Python.png"></img>
                    <h4 class="mb-0">Specify Process Noise and Covariance Matrices</h4> <!-------------------------->
                    <p>The process noise and the sensor noise covariance matrices need to be specified for the Kalman 
                        Filter to work well. Sigma_3 was initialized as 20. This value was determined from slide 35 in 
                        Lecture 13 as well as the ToF sensor datasheet. Sigma_Z represents the mistrust in the sensor readings.
                        The ToF sensor has a ranging error of +/- 20mm in ambient and dark lighting.
                    </p>
                        <img width="200" height="auto" src="Lab7/sigma3Calculation.png"></img>
                        <img width="350" height="auto" src="Lab7/sigma3Datasheet.png"></img>
                    <p>Sigma_1 and sigma_2 were initialized using the calculation found on slide 35 of Lecture 13. 
                        Sigma_U represents the mistrust in the Kalman Filter model. 
                        Sigma_1 is the relationship between the ToF distance reading and the model predictions.
                        Sigma_2 is the relationship between the velocity and the model predictions.
                    </p>
                        <img width="350" height="auto" src="Lab7/sigmas1_2Calculation.png"></img>
                    <p>In Jupyter Notebook, I calculated the sigma values for each dataset as shown in the below code. 
                        The initiallized sigma had a value of 20 for sigma_1 and 5 for sigma_2 as the robot is not moving 
                        initially.
                    </p>
                        <img width="400" height="auto" src="Lab7/sigmasPython.png"></img>
                    <p>The sigma values were altered as needed for each dataset.</p>
                    <h3 class="mb-0">5000 Level Task: Faster Frequency Kalman Filter -- And -- 
                        Implement and Test Kalman Filter: Python</h3> <!--------------------------------------------->
                    <p>The below section includes the 5000 level task to use a faster frequency and use the Kalman Filter
                        prediction step to estimate the state of the car, as well as the same data graphed without the
                        prediction step for comparison. To write the below code I went to office hours to ask questions 
                        and looked at all of the lab reports for the TAs. 
                    </p>
                    <p>The below code for the Kalman Filter function was provided in the lab instructions and implemented in 
                        Jupyter Notebook.
                    </p>
                        <img width="600" height="auto" src="Lab7/KF_Function_Python.png"></img>
                    <p>The first two lines finding mu_p and sigma_p are the prediction step of the Kalman Filter. If 
                        there is no new ToF sensor reading, then the returned mu and sigma will be equal to what is 
                        found during the prediction step. Within the if statement there is the update step of the Kalman
                        Filter that runs every time there is a new ToF reading. The current state is mu, the uncertainty
                        is sigma, the motor input values are u, and the distance readings are y.
                    </p>
                    <p>To loop through each time step the below code was used.</p> 
                        <img width="600" height="auto" src="Lab7/KF_PythonLoop.png"></img>
                    <p>The while loop determines how many 
                        times the Kalman Filter runs. How many times the while loop runs depends on the the second to 
                        last time step for a ToF sensor reading and the value of Delta_T. If the value of the ToF time 
                        array is less than or equal to the value of the index, i, then the value of newTOF will be set 
                        to 1 and the update step of the Kalman Filter will run. If the value of the ToF time array is 
                        not less than or equal to the value of the index, i, then the value of newTOF will be set to 
                        0 and only the prediction step of the Kalman Filter will run. The index i is updated by adding 
                        Delta_T each time the while loop runs and the value of i is appended to the array KF_time to plot 
                        the Kalman Filter. After i is updated, the Kalman Filter function is called and then the output
                        distance (x) is appended to the KF_distance array to later be plotted. The u input was the negative 
                        value of 40 subtracted from the motor array storing PWM values, divided by 215 (the upper bound 
                        of 255 minus the lower limit of 40). The lower limit PWM value is 40. In this model if the motor 
                        input value is 40 then the value for u is zero (-(40-40)/215). 
                    </p>
                    <h5 class="mb-0">Filter Parameters</h5> <!------------------------------------------------------->
                    <p>Throughout the lab report I discuss the parameters used to modify the Kalman Filter and in the section below I
                        discuss how the parameters are modified to better fit the Kalman Filter to the ToF distance data.
                        To summarize, the parameters include the sampling time (Delta_T), drag (d), momentum (m), and sigma values
                        including Sigma_1, Sigma_2, and Sigma_3. The sigma values are used to find the covariance matrices: 
                        Sigma_U and Sigma_Z. Sigma_U represents the confidence in the model (distance and velocity) and Sigma_Z
                        represents the confidence in the sensor measurement. Drag is inversely proportional to speed and momentum 
                        is related to how much effort is needed to change the speed of the robot.
                        In extension, the values for drag and momentum affect the 
                        A and B matrices. Delta_T and the A and B matrices affect the values of the Ad and Bd matrices. 
                    </p>
                    <h4 class="mb-0">Kp = 0.05, Ki = 0.000008, Kd = 8</h4> <!------------------------------->
                        <img width="400" height="auto" src="Lab7/Graph2/Graph2.png"></img>
                        <img width="250" height="auto" src="Lab7/Graph2/PID_Graph2.png"></img>
                    <p>Initially running the Kalman Filter results in the above graphs. 
                        The calculated Delta_T for the PID controller is 0.0058 seconds, a frequency of 172.41Hz. The Kalman 
                        Filter is running at a faster frequency than the ToF sensors. 
                        The maximum PWM value is 115. Sigma_1 and Sigma_2 are 131.54 and Sigma_3 is 20. Overall, the
                        model works well, but the predictions could be more inline with the ToF distances.
                        In the next step I modify the parameters to adjust the prediction values. 
                    </p>
                    <hr>
                        <img width="400" height="auto" src="Lab7/Graph2/FixedGraph2.png"></img>
                        <img width="400" height="auto" src="Lab7/Graph2/ZoomFG2.png"></img>
                        <img width="400" height="auto" src="Lab7/Graph2/Zoom2FG2.png"></img>
                        <img width="400" height="auto" src="Lab7/Graph2/Zoom3FG2.png"></img>
                    <p>I adjusted the parameters for Sigma_1, Sigma_2, and Sigma_3. Decreasing Sigma_1 to 10 (model uncertainty for location) 
                        and Sigma_2 to 120 (model uncertainty for velocity) indicates 
                        that the model is more accurate than what was initially predicted.
                        Sigma_3 (sensor uncertainty) was changed to 10 as the sensor readings were reliable. 
                        All other parameters (d, m, and Delta_T) were kept the same as the previous step.
                        As can be seen from the graphs the Kalman Filter more closely follows the trend of the distance measurements.
                    </p>
                        <img width="400" height="auto" src="Lab7/Graph2/ChangedZoomFG2.png"></img>
                    <p>In the above graph I kept all previously found parameters the same but changed the value of the increment "i"
                        in the while loop in the same manner as I did for Graph 1. The line changed from "i = i + Delta_T" to "i = i + 2*Delta_T". 
                        This change results in less Kalman Filter predictions to be plotted and the same offset behaviour as seen in Graph 1. 
                    </p>
    
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>
<!___________________________________________________________________________________________________________________>                    
<!__ Lab 6: Orientation PID Control _________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 6: Orientation PID Control</h2>
                    <p>The objective of the lab to to gain experience working with PID controllers for orientation. 
                        Labs 5-8 detail PID, sensor 
                        fusion, and stunts. This lab is focused on using a PID controller for orientation control of the 
                        robot. The IMU will be used to control the yaw of the robot.
                    </p>
                    <h3 class="mb-0">PID Control Explanation</h3> <!----------------------------------------------->
                        <p>For this lab a PID controller will be used for orientation control of the robot. The below 
                            equation from the course Lecture 7 is used to compute the value for the PID controller:
                        </p>
                        <img width="350" height="auto" src="Lab5/PIDEquation.png"></img>
                        <p>This equation uses the error e(t) (difference between the final desired value and the current
                            value) in proportional, integral, and derivative control to calculate the new PID input value u(t).
                            For this lab the error is the difference between the desired angle (yaw)
                            and the robots current angle (yaw). The calculated PID input value is related to the robots 
                            angular speed. Proportional control multiplies a constant Kp by the error. Integral control multiplies 
                            a constant Ki by by the integral of the error over time. Derivative control multiplies a 
                            constant Kd by the rate of change of the error.
                        </p>
<!-- Lab 6 Lab Tasks ----------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <h3 class="mb-0">PID Input Signal</h3> <!------------------------------------------------------->
                    <h4 class="mb-0">Integrate Gyroscope To Estimate Orientation</h4> <!---------------------------->
                    <p>The gyroscope reading was integrated using the following lines of code:</p>
                    <img width="550" height="auto" src="Lab6/Code/GyroInt.png"></img>
                    <p>These lines of code are within the larger while loop and each time the while loop runs the 
                        gyroscope updates the sensor readings. Yaw is then found using the previous yaw reading, the 
                        gyroscope reading, and the calculated difference in time.
                    </p>
                    <h3 class="mb-0">Orientation Control</h3> <!---------------------------------------------------->
                    <p>For the orientation control task the robot uses a PID controller and the calculated yaw angle 
                        from the gyroscope readings to move towards and maintain the set angle. The implemented PID 
                        controller works for various angles and on different floor surfaces.
                    </p>
                    <h3 class="mb-0">Rotational Lower Limit PWM</h3> <!--------------------------------------------->
                    <p>While I was initially testing the PID controller the lower limit PWM value was set to the same as the 
                        lower limit value of 40 from the previous lab. Using a lower limit of 40 resulted in the robot 
                        getting stuck and not fully turning. I created a new Arduino case and Jupyter Notebook command 
                        to find the lower limit PWM for turning.
                        After testing various PWM values, I found that a lower limit value of 120 resulted in the robot 
                        turning. 
                    </p>
                    <h3 class="mb-0">Proportional Control (Kp)</h3> <!---------------------------------------------->
                    <p>I started with testing the proportional controller. I worked on finding a value for Kp, while 
                        keeping Ki and Kd zero. I choose an angle of 180 degrees to start testing the proportional 
                        controller. I estimated the Kp constant to be 1.4167 when testing 180 degrees. The equations 
                        are found below:
                    </p>
                    <img width="300" height="auto" src="Lab6/Code/KpEquation.png"></img>
                    <p>During physical testing I wanted the proportional control to slightly over shoot the desired 
                        angle. The over shoot will be addressed using derivative control and the speed will be 
                        addressed using integral control. A range of Kp values including 1, 1.5, 1.75, and 2
                        were tested as can be seen in the below
                        graphs showing yaw, error, PID components, and motor input. I determined that the Kp value 
                        should be 2. The code for the proportional controller is below:
                        <script src="https://gist.github.com/MikaylaLahr/4e09fa35c335168ddcc79a11e495780d.js"></script>
                    </p>
                    <hr>
                    <p>Graphs for Kp = 2:</p>
                        <img width="400" height="auto" src="Lab6/Kp/Kp2Yaw.png"></img>
                        <img width="400" height="auto" src="Lab6/Kp/Kp2Error.png"></img>
                        <img width="400" height="auto" src="Lab6/Kp/Kp2PID.png"></img>
                        <img width="400" height="auto" src="Lab6/Kp/Kp2Motor.png"></img>
                    <hr>
                    <p>Trends that can be observed include, as the value for Kp increases the amount of time the robot 
                        takes to reach the desired angle decreases, but there is also more overshoot observed. As the 
                        value of Kp increases the motor input spends less time at the lower limit value of 120. For 
                        example, it is noticeably observed that for Kp = 1 the motor quickly reach the input value of 120 
                        and remains at that value until the desired angle is reached. For Kp = 2 the motors start at the 
                        upper bound of 255 and then decrease to the lower limit of 120 before reaching 180 degrees. I 
                        choose a value of Kp = 2 to then move to testing the proportional and integral controllers 
                        together.
                    </p>
                    <h3 class="mb-0">5000 Level Task: Integral Control (Ki) and Wind Up Protection</h3> <!---------->
                    <p>The next step involved using the proportional controller (with Kp = 2) and the integral controller 
                        to increase the speed of rotation and to achieve a small amount of oscillation at the desired angle. 
                        The integral controller is the constant Ki multiplied by integrated error over time. As time increases
                        the integrated error increases, resulting in a greater contribution of the integral controller. To 
                        prevent the integral controller component from increasing continuously, wind up protection was 
                        implemented. If the value of Ki mulitplied by the time integrated error is greater than 200 or less
                        then -200, then the KiIntegral is set to be 200 or -200 and does not further increase/decrease. The code for 
                        the proportional and integral controller is below.
                        <script src="https://gist.github.com/MikaylaLahr/7ed7019acdbd640b36c86cd33689b522.js"></script>
                    </p>
                    <p>Below are graphs for the tested values for Kp and Ki.</p>
                    <hr>
                    <p>Graphs for Kp = 2 and Ki = 0.05:</p>
                        <img width="400" height="auto" src="Lab6/Ki/Ki0.05Yaw.png"></img>
                        <img width="400" height="auto" src="Lab6/Ki/Ki0.05Error.png"></img>
                        <img width="400" height="auto" src="Lab6/Ki/Ki0.05PID.png"></img>
                        <img width="400" height="auto" src="Lab6/Ki/Ki0.05Motor.png"></img>
                    <hr>
                    <p>Trends that can be observed include, as the value for Ki increases the amount of oscillation 
                        and overshoot of the robot at the desired angle increases. The motor input for all tested 
                        values starts at the maximum of 255 before decreasing to a value of 120 before reaching the 
                        desired angle. For a value of Ki = 0.01 there is minimal overshoot and oscillation and a value
                        of Ki = 0.05 also results in minimal overshoot but slightly more oscillation. For Ki = 0.1
                        there is more oscillation before reaching the desired angle and for Ki = 0.2 the oscillation 
                        continues for a longer amount of time. I choose a value of Ki = 0.05 to move on to testing 
                        the proportional, integral, and derivative controllers together.
                    </p>
                    <h3 class="mb-0">Derivative Control (Kd)</h3> <!------------------------------------------------>
                    <h4 class="mb-0">Derivative Calculation: Lowpass Filter</h4> <!----------------------------------->
                    <p>The final derivative calculation used the previous gyroscope output combined with a low pass filter
                        with an alpha of 0.3055 to remove some of the noise. The calculation for alpha is found below 
                        along with the final code to find the derivative control.
                    </p>
                    <img width="400" height="auto" src="Lab6/Code/alpha.png"></img>
                    <p><script src="https://gist.github.com/MikaylaLahr/22d0a2a30f7bb8a852ab962ca8843466.js"></script>
                        I tested a serives of Kd values to find where the overshoot was reduced.
                        The final derivative calculation resulted in the spike and the noise being removed as can be seen 
                        in the below graphs. 
                    </p>
                    <hr>
                    <p>Graphs for Kp = 2, Ki = 0.05, and Kd = 0.05:</p>
                        <img width="400" height="auto" src="Lab6/Kd/Kd0.05Yaw.png"></img>
                        <img width="400" height="auto" src="Lab6/Kd/Kd0.05Error.png"></img>
                        <img width="400" height="auto" src="Lab6/Kd/Kd0.05PID.png"></img>
                        <img width="400" height="auto" src="Lab6/Kd/Kd0.05Motor.png"></img>
                    <hr>
                    <p>I choose a final value of Kd = 0.05 as the robot did not overshoot the desired angle of 
                        180 degrees.
                    </p>
                    <p>Below is a video of the robot turning +180 degrees at Kp = 2, Ki = 0.05, and Kd = 0.05:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/6YijdDLohiI"></iframe>
                    <h4 class="mb-0">More Videos</h4> <!------------------------------------------------------------>
                    <p>Below is a video of the robot at 0 degrees:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/i6WjCm1ocCw"></iframe>
                    <p>I also tested negative angles. Below is a video of the robot turning -90 degrees:</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/lYRn-KsOYhc"></iframe>
                   <p></p>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>                    
<!__ Lab 5: Linear PID Control and Linear Interpolation ____________________________________________________________>
                    <h2 class="mb-2.5">Lab 5: Linear PID Control and Linear Interpolation</h2>
                    <p>The objective of the lab to to gain experience working with PID controllers.
                        Labs 5-8 detail PID, sensor fusion, and stunts.
                        This lab is focused on using a PID controller for position control of the robot. 
                    </p>
                    <h3 class="mb-0">PID Control Explaination</h3> <!----------------------------------------------->
                        <p>For this lab a PID controller will be used for position control of the robot. The below 
                            equation from the course Lecture 7 is used to compute the value for the PID controller:
                        </p>
                        <img width="350" height="auto" src="Lab5/PIDEquation.png"></img>
                        <p>This equation uses the error e(t) (difference between the final desired value and the current
                            value) in proportional, integral, and derivative control to calculate the new PID input value u(t).
                            For this lab the error is the difference between the desired distance from the wall (1ft) 
                            and the robots current distance from the wall. The calculated PID input value is related to the robots 
                            speed. Proportional control multiplies a constant Kp by the error. Integral control multiplies 
                            a constant Ki by by the integral of the error over time. Derivative control multiplies a 
                            constant Kd by the rate of change of the error.
                        </p>
<!-- Lab 5 Lab Tasks ----------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                        <h3 class="mb-0">Final Code For While Loop And Added Functions</h3> <!---------------------->
                            <a href="https://drive.google.com/file/d/1yutieharWMii1eGlPbHo_Mj2dyzWmLrR/preview">Lab 5 Full Code</a>
                            <p>The above link has the full final version of the code used during the lab to collect 
                                distance measurements, extrapolate distance measurements, and move/stop the robot. 
                                I attached the link for my own reference in the future.
                                The below steps show how and why I wrote each piece of the code. 
                            </p>
                        <h3 class="mb-0">Position Control</h3> <!--------------------------------------------------->
                            <p>For the position control task the robot drives as fast as possible towards a wall and 
                                stops when it is 1ft away from the wall. The ToF sensor on the front of the robot will 
                                be used to determine the distance the robot is from the wall. The implemented PID control 
                                is also shown to be robust to changing conditions including starting distance and floor 
                                surface. 
                            </p>
                        <h4 class="mb-0">Proportional Control (Kp)</h4> <!------------------------------------------>
                            <p>The first control input I worked on was the Kp input for proportional control. I estimated 
                                the value to be around 0.06899 by using the below equation to solve for the PWM values. I wanted 
                                to minimize overshoot, so to find the value for the error I subtracted the final desired 
                                distance from the maximum error based on the ToF sensor readings. The equations used can 
                                be found below:
                            </p>
                            <img width="350" height="auto" src="Lab5/KpEquations.png"></img>
                            <p>The initial loop used to find the PID value is:</p>
                            <img width="400" height="auto" src="Lab5/NewPIDInitialLoop.png"></img>
                            <p>First the time difference (dt) is found and then the error is calculated based on the current 
                                distance and desired stopping distance. After the error is calculated the integral and 
                                derivative terms are solved for (both are 0 in this step). 
                                These PID equations were also used during the next step to find the Kd value for derivative
                                control.
                            </p>
                            <p>I used the calculated Kp value (0.06899) as an estimate. During the first physical testing of the system 
                                I tested a range of Kp values to determine 
                                at what point the robot oscillated around 1ft in front of the wall. Two of these Kp values are found graphed
                                below. I determined that the Kp value should be set 
                                to 0.03 to minimize the error and oscillation present.
                                The graphs showing Time vs ToF Distance, Time vs P Control, and Time vs Error 
                                for Kp = 0.03 can be found below: 
                            </p>
                            <p>Plots for Kp = 0.03:</p>
                            <img width="400" height="auto" src="Lab5/0.03KpToF.png"></img>
                            <img width="400" height="auto" src="Lab5/0.03KpP.png"></img>
                            <img width="400" height="auto" src="Lab5/0.03KpError.png"></img>
                            <p>Video for Kp = 0.03:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/_73SAd107NA"></iframe>
                            <p>
                                Larger Kp values resulted in the robot moving faster and having more error in the final 
                                position. Kp = 0.03 was the smallest value for Kp that resulted in the robot completing 
                                the full motion.
                                For this first task I 
                                wanted to reduce oscillation, rather than achieve a faster speed and moved to the next 
                                step using Kp = 0.03. I focused on achieving a faster speed when tuning the integral 
                                Ki value, and prevented overshoot when tuning the derivative Kd value.
                            </p>
         
                        <h4 class="mb-0">Derivative Control (Kd)</h4> <!-------------------------------------------->
                            <p>The next task I worked on was tuning the Kd value using proportional control and 
                                derivative control at the same time. I wanted to prevent overshoot and any oscillation 
                                of the robot. I increased the Kd value until the overshoot was reduced and the robot 
                                stopped at the 1ft desired 
                                distance from the wall. This Kd value was found to be Kd = 5. A video showing 
                                the robot at Kp = 0.03 and Kd = 5 is below:
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/gKO5rApL9Fc"></iframe>
                            <p>Derivative Kick and Low Pass Filter:</p>
                            <p>For this task the ending setpoint distance does not change, resulting in the error 
                                derivative to be continuous. I did not need to implement anything to get rid of derivative 
                                kick because there is no changing setpoint. I did consult the reference provided in the 
                                lab instructions and read about derivative kick 
                                <a href="http://brettbeauregard.com/blog/2011/04/improving-the-beginners-pid-derivative-kick/">on this website</a>, 
                                for future reference.
                            </p>
                            <p>In addition a low pass filter for the derivative was not implemented as all of the calculated 
                                derivative error values were small. 
                            </p>
                        <h4 class="mb-0">5000 Level Task: Integral Control (Ki) and Wind Up Protection</h4> <!------>
                            <p>For the next task I added the integral controller and inputed a Ki value. To start I set 
                                Kd to zero and tested a value of Ki = 0.00001 and Kp = 0.03. This resulted in the robot 
                                moving very fast into the wall. I then decided to decrease Ki until I found a value where 
                                the robot would stop in front of the wall, but for each value tested the robot ran into the wall.
                                I then implemented wind up protection for the integral control to prevent the robot from 
                                overshooting and running into the wall. 
                            </p>
                            <p>First a distance measurement is taken at the start of the robots motion to determine the 
                                total distance the robot needs to move and an integralStop value is set to be half of 
                                the total distance travelled. Each loop an if statement checks to see if the error is
                                greater than the integralStop value. If the error is greater then integralStop a new 
                                integral value will be calculated. If the value of the integral term multiplied by the 
                                set Ki value is greater than 100 then there will be no further increase in the KiIntegral 
                                term used to find the PID control. In additon, if the error is smaller then the integralStop value 
                                then the new integral value will be set to zero. The integral term is the Ki value multiplied by 
                                the time integrated error. Without wind up protection the integral term will continuously increase 
                                causing the robot to not stop when the final distance is reached. Below is a video showing the 
                                KiIntegral (I) terms continuously increase until stopping at 100. The error term is not changing 
                                significantly, but the KiIntegral (I) term continuously increases.
                            </p>
                            <p>I then added the Kd term found in the previous step. A value of Kd = 5 resulted in more 
                                overshoot than I wanted the robot to have. I increased the Kd value until the overshoot 
                                was again minimized. This resulted from a value of Kd = 8. For values above Kd = 8 there 
                                was no longer a noticable difference in minimizing overshoot. The behavior for Kp = 0.03, 
                                Ki = 0.000008, and Kd = 8 can be seen in the three trials below. The robot was started 
                                from a distance of 6.5ft.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/nLN_xBjYBLI"></iframe>
                            <p>The three below graphs show the data for Time vs ToF Distance, Time vs Motor Input, and
                                Time vs Error.
                            </p>
                            <img width="400" height="auto" src="Lab5/ErrorTrial1.png"></img>
                            <img width="400" height="auto" src="Lab5/NewMotorInputTrial1.png"></img>
                            <p>Graph Analysis and Deadband:</p>
                            <p>The above graphs for the distance and error reading show that there is minimal 
                                over shoot and the robot stops at the desired distance of 1ft away from the wall.
                                The graph for Time vs Motor Input shows that the initial motor input is 58 and 
                                decreases until a motor input of 40 is reached. The deadband for the motors is 40.
                                Any value between 0.5 and 40 (forward) or -0.5 and -40 (backward) will be set to 40
                                in order to produce a PWM value that causes the motors to move. I choose a lower bound 
                                of 0.5 and -0.5 to prevent the robot from overly moving after reaching the final distance.
                                The functions used to produce forward and backward motion were discussed previously in the 
                                section for Proportional Control. When the robot stops the motor input changes to zero. 
                            </p>
                            <p>Maximum Linear Speed:</p>
                            <p>The maximum linear speed of the robot can be found using the Time vs ToF Distance graph. Using 
                                two points on the graph it can be found that (1506mm - 707mm)/(36105ms - 38258ms) = 0.37m/s
                            </p>
                        <h3 class="mb-0">Extrapolation</h3> <!------------------------------------------------------>
                        <h4 class="mb-0">Frequency of Returned ToF Data</h4> <!------------------------------------->
                            <p>As stated previously in the lab in the Range and Sampling Time Discussion section, the 
                                sampling period was found to be 100ms, resulting in a sampling frequency of 
                                (1/0.1s) = 10Hz for distance measurements. 
                            </p>
                        <h4 class="mb-0">Speed of PID Control Loop and ToF Sensor Rate</h4> <!---------------------->
                            <p>As stated previously in the lab in the Range and Sampling Time Discussion section, The 
                                PID loop runs at around 8ms, which is faster than the ToF sensor produces new distance 
                                readings. Extrapolation will be used to estimate the distance when new data is not 
                                available using past ToF distance readings. 
                            </p>
                        <h4 class="mb-0">Calculate PID Control Every Loop and Extapolate Robot Distance From Past Data Readings</h4> <!------------------>
                            <p>Due to the ToF sensor running slower than the PID loop, extrapolation is used to estimate the distance
                                using past data points. Below is the extrapolation equation:
                            </p>
                            <img width="300" height="auto" src="Lab5/ExtrapolateEquation.png"></img>
                            <p>This equation was implemented in the Arduino code. An if statement checks if the ToF distance sensor 
                                has a new distance reading. If the sensor has a new distance reading then that point is added to 
                                the distance_data array and the current time and distance are stored in time1 and dis1 variables. 
                                The previous time and distance from the last time there was a new distance measurement are stored 
                                in the time2 and dis2 variables. If there is no new distance reading the extrapolation equation is 
                                used. 
                            </p>
                            <p>Below is a graph for the raw and extrapolated distances. The graph is zoomed in to show that the 
                                extrapolated data (orange) readjusts to the raw data reading (blue) every time there is a new distance 
                                measurement. In the next section I ran tests using the extrapolated data. 
                            </p>
                            <img width="400" height="auto" src="Lab5/ExtrapolatedGraph.png"></img>
                            <p></p>

<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__ Lab 4: Motors and Open Loop Control ___________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 4: Motors and Open Loop Control</h2>
                        <p>The objective of the lab is to control the car using open loop control, rather than the 
                            manual controller. Two dual motor drivers will be connected to the car's motors and the 
                            Artemis board. The dual motor drivers will be used to control the movement of the car.
                        </p>
                    <h3 class="mb-0">Sketch Of Wiring Diagram</h3> <!----------------------------------------------->
                        <p>Below is a wiring diagram for the two dual motor drivers connected to the Artemis board, 
                            DC Motors, and 850mAh battery. The two VIN pins and the two GND pins on both dual motor 
                            drivers are connected together and then connected to the 850mAh battery. The GND pin of 
                            one of the dual motor drivers is connected to the GND of the Artemis board. There is a 
                            bridge between the BOUT1 and AOUT1 pins of the dual motor driver that is then connected 
                            to the DC motor. There is also a bridge between the BOUT2 and AOUT2 pins of the dual motor 
                            driver that is then connected to the DC motor. There is a bridge between the BIN1 and AIN1 
                            pins on the dual motor driver and there is also a bridge between the BIN2 and AIN2 pins. 
                            On the dual motor driver that connects the drivers to the ground of the Artemis board (top 
                            dual motar driver in sketch below), the bridge between BIN1 and AIN1 is connected to pin 11, 
                            and the bridge between BIN2 and AIN2 is connected to pin 12. On the other dual motor driver 
                            (bottom dual motor driver in sketch below) the bridge between BIN1 and AIN1 is connected 
                            to pin 7, and the bridge between BIN2 and AIN2 is connected to pin 9. The pins used in this 
                            lab were chosen from the remaining pins on the Artemis board (after the previous lab). The 
                            parallel-couple of the two inputs and two outputs on each dual motor driver allows for 
                            twice the average current to be delivered without overheating.
                        </p>
                    <img width="550" height="auto" src="Lab4/DualMotorDriverWiring.png"></img>
                
<!-- Lab 4 Lab Tasks ----------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                        <h3 class="mb-0">Testing The Circuit With The Oscilloscope and Power Supply</h3> <!--------->
                        <h4 class="mb-0">Power Supply Setting And Oscilloscope Discussion</h4> <!--------------------->
                            <p>The voltage of the 850mAh battery is 3.7V. In place of using the battery, a power supply 
                                was used. The output of the power supply was set to 3.7V to represent the battery 
                                power that will be used later on during the lab. Each pin was tested independently of 
                                each other. In the Arduino IDE code, one pin was set to have an output of 200, while 
                                the other pins all had an output of 0. This was repeated for all four pins. The positive 
                                output voltage to the motor was displayed on the oscilloscope and indicated a constant 
                                voltage output at each time. The PWM values show that power can be regulated on the motor 
                                driver output.
                            </p>
                            <p>Channel 3 of the power supply was used. 3.7V was used to test the motor driver circuit.</p>
                            <img width="550" height="auto" src="Lab4/PowerSupply.png"></img>
                            <p>Picture of the oscilloscope output:</p>
                            <img width="550" height="auto" src="Lab4/OscilloscopeMonitor.png"></img>
                          <p></p>
                        <h3 class="mb-0">Testing The Robot Wheels</h3> <!------------------------------------------->
                        <h4 class="mb-0">Video Of Wheels Spinning (Each Side Of Robot)</h4> <!---------------------->
                            <p>The wheels on each side of the robot were tested individually at two different output 
                                values. The power supply used at the beginning of the lab (3.7V) was used to power 
                                the motors. Pins 7, 9, 11, and 12 were tested at output values of  50 and 200. While 
                                testing each side of the robot I observed the direction and speed that each wheel 
                                turned. The wheels rotated faster for the higher output at 200 compared to the output 
                                at 50. For the higher output value of 200, the wheels reached the final set speed faster 
                                compared to the lower set speed. When rotating at 50 the wheels took longer to start rotating.
                                From the perspective of the videos pin 7 rotates counter clockwise and pin 9 
                                rotates clockwise. Pin 7 and pin 9 move the wheels on the same side of the robot in 
                                different directions. In addition, from the perspective of the videos pin 11 moves 
                                clockwise and pin 12 moves counter clockwise. Pin 11 and pin 12 move the wheels on 
                                the same side of the robot in different directions. To consolidate the report, all 
                                of the videos taken can be found in this folder: <a href="https://drive.google.com/drive/folders/1C0Mv_Gn857_GPnvRFywwlARlJ5vl_ICM?usp=sharing">Folder Link</a>
                                Below are videos of pin 12 and pin 9 with an output value of 200. 
                            </p>
                            <p>Pin 12 with an output of 200:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/NtxG2B53yNE"></iframe>
                            <p>Pin 9 with an output of 200:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/vF0gQiVKP0E"></iframe>
                            <p>The Arduino code to test the wheels is found below. One pin was set to have an output 
                                value of either 50 or 200 and all other pins were set to have an output value of 0.
                                This is the same code that was used in the previous section to test if the circuit 
                                was working properly before sautering to the motors.
                            </p>
                        <h4 class="mb-0">Video Of All Wheels Spinning</h4> <!--------------------------------------->
                            <p>After testing each side of the robot, all of the wheels were tested at the same time. 
                                The 850mAh battery was used to power the motors instead of the power supply. The below 
                                video shows both sides rotating while the motors and Artemis board are both connected
                                to their respective batteries.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/riR7c_fM4ns"></iframe>
                            <p>The Arduino code to test the wheels on both sides of the robot is found below. Pins 9 
                                and 12 were set to have an output of 200 and pins 7 and 11 were set to have an output 
                                value of 0. This is the same code that was used in the previous section to test both 
                                sides of the robot individually.
                            </p>
                        <h4 class="mb-0">Picture Of All Components On Car</h4> <!----------------------------------->
                            <p>Below is a labeled picture showing all components on the robot. Including the 650mAh 
                                battery, Artemis board, IMU, two ToF sensors, two motor drivers, and a breakout board.
                                The 850mAh battery to power the motors is on the other side of the robot. 
                            </p>
                            <img width="550" height="auto" src="Lab4/RobotDiagram.png"></img>
                    
                            <p>Below is a diagram of the entire wiring that is used on the robot:</p>
                            <img width="600" height="auto" src="Lab4/CompleteWiring.png"></img>
                            <p></p>
                        <h3 class="mb-0">Lower Limit PWM And Open Loop Control</h3> <!------------------------------>
                        <h4 class="mb-0">Lower Limit PWM Value Discussion</h4> <!----------------------------------->
                            <p>The lower limit PWM value is the lowest output value that a pin can be set to that will 
                                still result in continuous motion of the robot. The wheels on one side of the robot 
                                were tested, while the wheels on the other side remained stationary. The lower limit 
                                PWM for pin 11 (right side) was found to be 59. The lower limit PWM for pin 7 (left
                                side) was found to be 50. 
                            </p>                       
                            <p>Pin 11 (Right Wheels) with an output of 59:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/j5dqoIw6FOU"></iframe>
                            <p>Pin 7 (Left Wheels) with an output of 50:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/ZOekIthGxmg"></iframe>
                            <p>I also found the PWM value for both wheels when they are being used together, starting 
                                when the robot is stationary. The output value was found to be 35 for both wheel sides. 
                                The video showing the robot moving when both wheels (pins 11 and 7) have an output 
                                value of 35.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/bWIED69jH9o"></iframe>  
                          
                        <h4 class="mb-0">Calibration Demonstration</h4> <!------------------------------------------>
                            <p>The wheels of the robot do not spin at the same rate and a calibration factor needed 
                                to be implemented. The below video shows the robot moving in a straight line, starting
                                at 7 feet and then continuing past the end of the tape measure. For the demonstration,
                                pin 11 was set to an output of 60 and pin 7 was set to an output of 70. In addition to 
                                the below video, the robot was run multiple times at varying input values. My first 
                                tested value was pin 11 at an output of 60, which is close to the found PWM value from 
                                the previous step. When pin 7 was set to an output of 51, the robot continuously moved 
                                towards the left. Through trial and error I found that an output of 70 for pin 7 kept 
                                the robot moving in a straight line. I used these found values for reference during 
                                future tests: 70/60=1.166666666666667. 
                            </p>
                            <p>I increased the output of pin 11 by 20 for each 
                                test and multiplied the value for pin 7 by 1.166666666666667. This incrementation worked 
                                for all tested pin 11 and pin 7 values until I reached an output value of 220 for pin 11. 
                                The calculated output value for pin 7 should be 257, rounded down to 255 due to 255 being
                                the maximum output value. For this test the robot continuously moved towards the right 
                                and the value for pin 7 needed to be lowered to 250 to keep the robot moving straight. 
                                Due to this discovery, I decided to also test an output value of 210 for pin 11, the 
                                calculated output value for pin 7 should be 245. In order to keep the robot moving 
                                straight, and not to the right I needed to reduce the output of pin 7 to 240. The trend 
                                shows that after an output of 200 for pin 11 the calibration factor to find the 
                                corresponding output value for pin 7 starts to differ. 
                            </p>
                            <p>In addition, as a final test I 
                                wanted to determine how low I could set the output values while still having the robot 
                                move in a straight line for at least 6 feet. I set the output value of pin 11 to 40 and 
                                calculated the output value of pin 7 to be 47. For these output values, the robot moved 
                                in a straight line for more than 6 feet. When I set the output value for pin 11 to be 
                                lower than 40 the wheels began to stall and hesitate before rotating compared to the other 
                                side of the robot. I also included a video at the end of the section showing the wheels 
                                spinning.
                            </p>
                            <p>To produce the robots motion the same Arduino IDE case and Jupyter Notebook command were 
                                used from the previous section. The length of the delay was changed depending on how fast the robot moved (to prevent 
                                the robot from running into a wall). All of the recorded videos are in <a href="https://drive.google.com/drive/folders/1MPB9ob9C-Wnmy3WKEV63vXBIjMhBrcxV?usp=drive_link">this folder.</a>
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/up1g-nOJNPY"></iframe>
                            <p>Below is a graph showing all of the tested values as data points. In addition, I plotted the 
                                trendline for all data points, as well as the trendline for all of the data points except 
                                for the last two that began to differ from the previous measurements. The slope and 
                                y-intercept were also found for each trendline. The calculated slope is the calibration 
                                factor. The slope excluding the last two data points, 1.165 is close to the 1.166666666666667
                                value.
                            </p>
                            <img width="550" height="auto" src="Lab4/NewStraightLineGraph.png"></img>
                            <p>Video showing wheels hesitating when pin 11 (right) has an output of 35 and pin 7 (left)
                                has an output of 41:
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/T0syacfa-Sk"></iframe>
                            <p>I also tested the calibration factor when the robot is going in "reverse" (with the robot 
                                moving in the direction away from the ToF sensor on one end). I initially tested the 
                                previously found output values with the corresponding pins on each wheel side, with pin 12 
                                having an output of 60 and pin 9 having an output of 70. I found that by doing this the 
                                robot moved towards the left. I decided to switch the output values; pin 12 has an output 
                                of 70 and pin 9 has an output of 60). The robot moves straight using these output values. 
                                I find this interesting as it confirm my previous thoughts about one side of the wheels 
                                being slightly misaligned. 
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/KikE52expXI"></iframe>
                        <h4 class="mb-0">Open Loop Code And Video</h4> <!------------------------------------------->
                            <p>Below is a video showing open loop control of the robot. In the video the robot moves 
                                forward, turn towards the right, moves backward, stops, spins counterclockwise, stops,
                                spins clockwise, and then stops. I used output values found from the calibration graph.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/OKhd_hvgN-8"></iframe>
                          <p></p>
     
                        <h4 class="mb-0">Lowest PWM Value Speed Discussion</h4> <!---------------------------------->
                            <p>The lowest PWM value for each side of the robot was found by running the robot with an 
                                initial output of 60 for pin 11 and 70 for pin 7 to correspond with the found 
                                calibration values. The robot ran with these outputs for 3 seconds and then all pin 
                                outputs were set to zero expect either pin 11 or pin 7. Either pin 11 or pin 7 remained 
                                turned on for another 3 seconds, but decreased to a lower output value. I ran the robot 
                                multiple times to determine at what output value the robot no longer moved. It was 
                                determined that for pin 11 the lowest PWM output is 35 and for pin 7 the lowest PWM 
                                output is 33. The videos for each pin can be found below.
                            </p>
                            <p>Pin 11 with a lowest PWM value of 35:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/GVot4WoKJfg"></iframe>
                            <p>Pin 7 with a lowest PWM value of 33:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/y8LbCTM5KOg"></iframe>
                            <p>In addition, the lowest combined wheel PWM value was found when both wheels were being 
                                used at the same time. The output value for both wheels (pin 11 and pin 7) is 25 when 
                                the robot is already moving. The robot was initially run with an initial output of 60 
                                for pin 11 and 70 for pin 7 for 1.5 seconds and then both wheels decreased to an output 
                                of 25 for 5 seconds. Below are two videos. One video shows the final output at 25 (the 
                                robot moves until the total set time of 5 seconds is over). The other video shows the 
                                robot stop before the 5 seconds is over and the motors stall when set to an output of 
                                24, confirming the lower limit PWM value of 25 to keep the robot moving.
                            <p>Both Wheels: Lower Limit PWM of 25:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/Qsuw1Hp_cbg"></iframe>
                            <p>Both Wheels: Tested value of 24:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/8UgnAjQbpKM"></iframe>
        <p></p>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__ Lab 3: TOF ____________________________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 3: TOF</h2>
                    <p>The objective of the lab is to connect the time of flight (TOF) sensors to the Artemis board. 
                    </p>
                    <h3 class="mb-0">Task 1: Note The I2C Sensor Address</h3> <!------------------------------------>
                        <p>From the datasheet it was found that the I2C sensor address for the ToF sensor is 0x52. 
                            The two ToF sensors have the same address and one of the addresses will need to be changed 
                            in order to receive data from both sensors at the same time. 
                        </p>
                    <h3 class="mb-0">Task 2: Approach To Using Two ToF Sensors</h3> <!------------------------------>
                        <p>To use two ToF sensors, the XSHUT pin will be used to connect one of the ToF sensors to pin 
                            8 on the Artemis board. The XSHUT pin will be set to LOW and then the I2C sensor address 
                            will be changed for the other ToF sensor that is on. After the I2C address has been changed 
                            the XSHUT pin will be set to HIGH, turning the other TOF sensor back on. This will allow for 
                            both ToF sensors to be used in parallel. 
                        </p>
                    <h3 class="mb-0">Task 3: Placement Of Sensors On Robot</h3> <!---------------------------------->
                        <p>The placement of the two ToF sensors on the robot will impact the field of vision. Two 
                            long QWIIC connectors will be used to attach the ToF sensors. Longer connectors will allow 
                            me to position the ToF sensors in more potential ways compared to shorter wires. The 
                            potential arrangements of the sensors include (1) a sensor on the front and left side, 
                            (2) a sensor on the front and right side, (3) two sensors positioned on either side of 
                            the front of the robot, and (4) a sensor on the front and the back. Depending on the task 
                            at hand any of these sensor positions may be effective. 
                            If there is no sensor on the back of the robot there might be an issue if the robot needs 
                            to back up or move in reverse. If there is no sensor on the side then the robot might miss 
                            an obstacle that is located in a blind spot. 
                        </p>
                    <h3 class="mb-0">Task 4: Sketch Of Wiring Diagram</h3> <!--------------------------------------->
                        <img width="550" height="auto" src="Lab3/Lab3WireDiagram.jpg"></img>
<!-- Lab 3 Lab Tasks ----------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                        <h3 class="mb-0">Task 1: ToF Sensor Connected To QWIIC Breakout Board</h3> <!--------------->
                            <p>The Artemis board was powered using a battery:
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/DU1Uwm4gzos"></iframe>
                            <p>ToF sensors connected to the QWIIC breakout board:</p>
                            <img width="550" height="auto" src="Lab3/Lab3Setup.jpeg"></img>
                            <hr>
                            <img width="400" height="auto" src="Lab3/Lab3ToF1.jpeg"></img>
                            <hr>
                            <img width="400" height="auto" src="Lab3/Lab3Tof2.jpeg"></img>
                        <h3 class="mb-0">Task 2: Artemis Scanning For I2C Device</h3> <!---------------------------->
                            <p>The SparkFun VL53L1X 4m Laser Distance Sensor Library was installed on Arduino IDE.
                                The code to find the I2C address: File->Examples->Apollo3->Wire->Example05_Wire_I2C.ino
                            </p>
                            <img width="550" height="auto" src="Lab3/Lab3I2C.png"></img>
                            <p>One ToF sensor was connected during the scan. The sensor had an I2C address of 0x29. 
                                In the datasheet for the ToF sensor, the listed I2C address is 0x52. The 
                                Example05_Wire_I2C.ino code returned an address of 0x29. This is a result of the 
                                binary for address 0x52 (binary: 0b01010010) being shifted to the right, resulting in 
                                an address of 0x29 (binary: 0b00101001). The rightmost bit is ignored because the bit 
                                identifies if data is read/write.
                            </p>
                        <h3 class="mb-0">Task 3: Sensor Data With Chosen Mode</h3> <!------------------------------->
                            <h4 class="mb-0">Sensor Mode Discussion</h4> <!----------------------------------------->
                            <p>The ToF sensor has two modes: (.setDistanceModeMedium(); //3m has been removed)
                                <li>.setDistanceModeShort(); //1.3m</li>
                                <li>.setDistanceModeLong(); //4m</li>
                            </p>
                            <p>Information from the datasheet shows 
                                that the short mode can only reach up to 1.3m and is the most resistant to ambient 
                                light. The long mode can reach up to 4m but is very sensitive to ambient light, which 
                                decreases the distance to 0.73m. The timing budget range for the short mode is 20ms - 
                                1000ms. The timing range for the long mode is 33ms - 1000ms. Short mode has greater 
                                accuracy and less sensor noise compared to the long mode. While it would be beneficial 
                                for a robot that needs to sense objects farther away to use the long mode, for this lab 
                                I am prioritizing accuracy and choose to test the short mode. 
                            </p>
                            <h4 class="mb-0">Testing .setDistanceModeShort();</h4> <!------------------------------->
                            <p>The ToF sensor was first tested using the 
                                File->Examples->SparkFun_VL53L1X_4m_Laser_Distance_Sensor->Example1_ReadDistance.ino
                                This code was modified to include the line 
                                distanceSensor.setDistanceModeShort(); in the setup to choose the short mode for the 
                                ToF sensor.
                            </p>
                            <p>ToF Sensor Setup: 
                            </p>
                            <img width="500" height="auto" src="Lab3/Lab3DistanceSetup1.jpeg"></img>
                            <img width="500" height="auto" src="Lab3/Lab3DistanceSetup2.png"></img>
                            <hr>
                            <p>Arudino code to find the ToF distance: 
                            </p>
                            <img width="650" height="auto" src="Lab3/Lab3DistanceArduino.png"></img>
                            <img width="550" height="auto" src="Lab3/DistanceJupyter.png"></img>
                            <hr>
                            <p>Sensor Range:</p>
                            <p>In the datasheet it is stated that the short mode for the ToF sensor has a range up to 1.3m.
                                To test the range of the sensor I took an initial measurement at 100mm and continued to take 
                                measurements up to 1900mm. The graph below shows the actual distance of the ToF sensor from 
                                the wall vs the ToF sensor distance.
                                The plotted measured distance value is the mean value of the ten collected data 
                                points. From the graph below it can be seen that after 1.3m the measured values start to 
                                differ more from the actual values compared to measured values less than 1.3m. 
                            </p>
                            <img width="550" height="auto" src="Lab3/Lab3MeasuredvsActual.png"></img>
                            <hr>
                            <p>Accuracy:</p>
                            <p>The difference in the measured and actual distances was plotted against the actual distances the 
                                ToF sensor was from the wall. When the ToF sensor is less than 1.3m from the wall, the difference 
                                is between 18mm and 40mm. After 1.3m the difference in the measured vs actual distance increases. 
                            </p>
                            <img width="550" height="auto" src="Lab3/Lab3DiffvsActual.png"></img>
                            <hr>
                            <p>Repeatability:</p>
                            <p>The standard deviation vs actual distance graph is shown below. The standard deviation is below 3mm 
                                for measurements under 1.3m. After 1.3m the standard deviation starts to vary and the standard 
                                deviation at 1900mm is 8.75mm.
                            </p>
                            <img width="550" height="auto" src="Lab3/Lab3SDvsActual.png"></img>
                            <p>Below are the time and measured distance arrays:</p>
                            <p>100mm: 9 measurements at 75mm, 1 measurement at 74mm</p>
                            <img width="550" height="auto" src="Lab3/D100mm.png"></img>
                            <hr>
                            <p>300mm: 6 measurements at 268mm, 4 measurements at 267mm</p>
                            <img width="550" height="auto" src="Lab3/D300mm.png"></img>
                            <hr>
                            <p>500mm: 5 measurements at 478mm, 5 measurements at 479mm</p>
                            <img width="550" height="auto" src="Lab3/D500mm.png"></img>
                            <hr>
                            <p>700mm: 5 measurements at 679mm, 4 measurements at 681mm, 1 measurement at 677mm</p>
                            <img width="550" height="auto" src="Lab3/D700mm.png"></img>
                            <hr>
                            <p>900mm: 4 measurements at 882mm, 5 measurements at 879mm, 1 measurement at 883mm</p>
                            <img width="550" height="auto" src="Lab3/D900mm.png"></img>
                            <hr>
                            <p>1100mm: 5 measurements at 1076mm, 4 measurements at 1070mm, 1 measurement at 1071mm</p>
                            <img width="550" height="auto" src="Lab3/D1100mm.png"></img>
                            <hr>
                            <p>1300mm: 4 measurements at 1258mm, 4 measurements at 1263mm, 2 measurements at 1265mm</p>
                            <img width="550" height="auto" src="Lab3/D1300mm.png"></img>
                            <hr>
                            <p>1500mm: 3 measurements at 1448mm, 2 measurements at 1447mm, 3 measurements at 1440mm, 2 measurements at 1449mm</p>
                            <img width="550" height="auto" src="Lab3/D1500mm.png"></img>
                            <hr>
                            <p>1700mm: 2 measurements at 1634mm, 3 measurements at 1631mm, 3 measurements at 1632mm, 2 measurements at 1637mm</p>
                            <img width="550" height="auto" src="Lab3/D1700mm.png"></img>
                            <hr>
                            <p>1900mm: 3 measurements at 1782mm, 2 measurements at 1791mm, 4 measurements at 1802mm, 1 measurement at 1784mm</p>
                            <img width="550" height="auto" src="Lab3/D1900mm.png"></img>
                            <hr>
                            <p>Ranging Time:</p>
                            <p>The ranging time is the time between each sensor measurement. I commented out all print 
                                statements, except the calculated value between the start and end times to allow for 
                                code to run as fast as possible. The ranging time was found to be around 55ms. 
                            </p>
                            <img width="400" height="auto" src="Lab3/Lab3RangingTime.png"></img>
                            <hr>
                        <h3 class="mb-0">Task 4: Two ToF Sensors: Working In Parallel</h3> <!----------------------->
                            <p>Two ToF Sensors were connected to the Artemis board and Arduino code needed to be 
                                written for the two ToF sensors to work in parallel. The code below is written 
                                according to the prelab procedure. 
                            </p>
                            <img width="550" height="auto" src="Lab3/2TOFCode.png"></img>
                            <p>The Serial Monitor prints the I2C addresses for both distance sensors to confirm 
                                that one of the addresses was changed to 0x30 and the other remains 0x52. 
                            </p>
                            <img width="450" height="auto" src="Lab3/2TOFDistanceSerial.png"></img>
                            <hr>
                            <iframe width="550" height="400" src="https://youtube.com/embed/g8wL6XBF1ZU"></iframe>
                        <h3 class="mb-0">Task 5: ToF Sensor Speed: Speed And Limiting Factor</h3> <!---------------->
                            <p>The code to find the ToF sensor speed can be found below. Time is continuously printed 
                                and distance measurements are only printed when a measurement is available. 
                            </p>
                            <img width="550" height="auto" src="Lab3/TimeDistanceCode.png"></img>
                            <p>When the 
                                sensors are not collecting data the loop takes around 2ms. When data is being collected 
                                the loop takes around 11ms. The longer loop time when collecting ToF sensor data is 
                                the limiting factor. The limiting factor is the time needed to record distance 
                                measurements.
                            </p>
                            <img width="400" height="auto" src="Lab3/TimeDistanceSerial.png"></img>
                            <hr>
                            <iframe width="550" height="400" src="https://youtube.com/embed/8FiNLwD_AbY"></iframe>
                        <h3 class="mb-0">Task 6: Time vs Distance: Data From Two ToF Sensors Over Bluetooth</h3> <!->
                            <p>The code from Lab 2 was modified to send time stamped ToF sensor data to Jupyter 
                                Notebook. The data for the two ToF sensors was plotted against the recorded time. The 
                                case I created in Arduino can be found below. 
                            </p>
                            <img width="650" height="auto" src="Lab3/Lab3ToFBluetoothCode.png"></img>
                            <hr>
                            <img width="550" height="auto" src="Lab3/Lab3ToFTimeGraph2.png"></img>
                            <hr>
                            <img width="550" height="auto" src="Lab3/Lab3TwoSensorsArray.png"></img>
                        <h3 class="mb-0">5000 Level Task 1: Discuss Different Distance Infrared Sensors</h3> <!----->
                            <h4 class="mb-0">Active and Passive Infrared Sensors</h4>
                            <p>The two main types of infrared sensors are active and passive infrared sensors. Passive 
                                infrared sensors sense nearby emitted infrared light, using a difference in signals 
                                between two pyroelectric sensors. These sensors are often used for security 
                                purposes to detect if there is a heat energy change in the environment, but cannot 
                                determine the distance to an object.
                                Active infrared sensors use an emitter and receiver to determine the distance
                                to an object.  
                                If there is an obstruction, the active infrared sensor knows that there is an object 
                                in between the emitter and receiver. Active infrared sensors are used in industry 
                                conveyor belts, as well as in garage door safety mechanisms. 
                                <a href="https://www.arrow.com/en/research-and-events/articles/understanding-active-and-passive-infrared-sensors">(Source: Arrow Electronics)</a>
                                Different types of infrared sensors include proximity sensors, ToF sensors, and triangulation sensors.
                                <a href="https://itp.nyu.edu/physcomp/distance-sensors-the-basics/#:~:text=Infrared%20LED%20Proximity%20Sensors,'%20rather%20than%20'distance'.">(Source: NYU)</a>
                            </p>
                        <h3 class="mb-0">5000 Level Task 2: ToF Response To Different Colors And Textures</h3> <!--->
                            <p>Four colors and three textures were tested at a distance of 500mm. Colors include pink, 
                                blue, yellow, and red. Textures include a hand towel, rug, and folder. The control 
                                surface was the dark grey wall that I used throughout the lab for distance readings.
                                The change in color or texture did not affect the ToF sensor's distance output that 
                                remained around 478mm.
                            </p>
                            <p>Control:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/X29VpoKLW5k"></iframe>
                            <hr>
                            <p>Color Pink:</p>
                            <iframe width="550" height="400" src="https://drive.google.com/file/d/1PUoaYNY344HtOt2QB5BEjrhq8XYqkNpa/preview"></iframe>
                            <hr>
                            <p>Color Blue:</p>
                            <iframe width="550" height="400" src="https://drive.google.com/file/d/1jj8VRlJxJkV5Udfwd45NXSNasH43Xe9T/preview"></iframe>
                            <hr>
                            <p>Color Yellow:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/TF_1-f9MVUo"></iframe>
                            <hr>
                            <p>Color Red:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/IE-Uqex8R2I"></iframe>
                            <hr>
                            <p>Texture Hand Towel:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/FFUvzX8Frlc"></iframe>
                            <hr>
                            <p>Texture Rug:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/rp7Hfwz03OA"></iframe>
                            <hr>
                            <p>Texture Folder:</p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/s5__EZTu5o8"></iframe>
                        </div>
                    </div>
<!-- Lab 3 References ---------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab 3 References</h2>
                <p>Thank you to all of the TAs that answered my questions. I referenced the past lab reports of 
                    Liam Kain, Rafael Gottlieb, Larry Lu, Julian Prieto, and Ignacio Romo.
                </p>
        </div>
    </section>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__ Lab 2: IMU ____________________________________________________________________________________________________>
                    <h2 class="mb-2.5">Lab 2: IMU</h2>
                    <p>The objective of the lab is to connect the IMU to the Artemis board and record the robot 
                        performing a stunt.
                    </p>
<!-- Lab 2 Prelab -------------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Prelab</h2>
                    <p>The prelab involved reading about the IMU used in the lab. The lab materials include: 
                        <li><a href="https://www.sparkfun.com/products/15443">SparkFun RedBoard Artemis Nano</a></li>
                        <li><a href="https://www.amazon.com/SUMPK-Charging-Braided-Compatible-Samsung/dp/B09Z6J21LY/ref=sr_1_4?keywords=usb%2Bc%2Bto%2Bc&qid=1636380583&qsid=147-6677549-1776715&refinements=p_n_feature_ten_browse-bin%3A23555327011&rnid=23555276011&s=pc&sr=1-4&sres=B08D9SB161%2CB08R68T84N%2CB01CZVEUIE%2CB01FM51812%2CB07VCZV3R4%2CB075V68NVR%2CB075GMKZWW%2CB093BVBRJT%2CB09BBBJ33F%2CB09C2D9Z7T%2CB012V56D2A%2CB092CYFQMP%2CB081L4V3DN%2CB07Y6ZJT1D%2CB07Y2XKPX5%2CB07VPYJV8V%2CB07THJGZ9Z%2CB08W2TP2TT%2CB0744BKDRD%2CB07THFJ1J5&srpt=ELECTRONIC_CABLE&th=1">UCB-C to USB-C Cable</a></li>
                        <li><a href="https://www.sparkfun.com/products/15335">SparkFun 9DOF IMU Sensor</a> and <a href="https://cdn.sparkfun.com/assets/7/f/e/c/d/DS-000189-ICM-20948-v1.3.pdf">IMU Datasheet</a></li>
                        <li><a href="https://force1rc.com/products/cyclone-remote-control-car-for-kids-adults">Force 1 RC Car</a></li>
                        <li><a href="https://www.amazon.com/URGENEX-Battery-Rechargeable-Quadcopter-Charger/dp/B08T9FB56F/ref=sr_1_3?keywords=lipo+battery+3.7V+850mah&qid=1639066404&sr=8-3">Li_Ion 3.7V 850mAh Battery</a></li>
                    </p>
<!-- Lab 2 Lab Tasks ----------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                        <h3 class="mb-0">Task 1: Set Up The IMU</h3> <!--------------------------------------------->
                        <h4 class="mb-0">Install SparkFun</h4>
                        <p>The SparkFun 9DOF IMU Breakout - ICM 20948 - Arduino Library was installed. 
                        </p>
                        <img width="500" height="auto" src="Lab2/IMULibrary.png"></img>
                        <hr>
                        <h4 class="mb-0">Artemis IMU Connections</h4>
                        <p>The setup for this lab is shown in the image below. 
                        </p>
                        <img width="450" height="auto" src="Lab2/Lab2Setup.jpeg"></img>
                        <hr>
                        <h4 class="mb-0">IMU Example Code and AD0_VAL</h4>
                        <p>The example code found under 
                            SparkFun 9DOF IMU Breakout - ICM 20948 - Arduino Library->Arduino->Example1_Basics 
                            was run to test if the IMU was working correctly. 
                        </p>
                        <p>Upon opening the Example1_Basics code, the AD0_VAL default was set to one. When the ADR 
                            jumper is closed the value should be set to zero. AD0_VAL corresponds to the last bit 
                            of the I2C address. My ADR jumer on the IMU was closed. For this reason, I needed to 
                            change the AD0_VAL to zero to return the accelerometer, gyroscope, magnetometer, and 
                            temperature data.
                        </p>
                        <p>While viewing the output data 
                            in the serial monitor, I moved the IMU to change the accelerometer and gyroscope data. I 
                            looked at the notated axis that was printed on the IMU and moved the IMU along the X, 
                            Y, and Z axis. I noticed that the respective X, Y, and Z serial monitor outputs for the 
                            accelerometer increased 
                            when I moved the IMU along the respective positive axis. The gyroscope data for pitch, roll, 
                            and yaw also changed when I rotated the IMU around the X, Y, and Z axis. 
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/W4k1yphP308"></iframe>
                        <img width="750" height="auto" src="Lab2/Example1ArduinoPicture.png"></img>
                        <hr>
                        <h4 class="mb-0">Adding A Slow Blink</h4>
                        <p>In the Example1_Basics code, I added a slow blink at the beginning of void setup() to 
                            turn the Artemis LED on and off three times. 
                        </p>
                        <img width="300" height="auto" src="Lab2/Example1AddedBlinkArduino.png"></img>
                        <hr>
                        <iframe width="550" height="400" src="https://youtube.com/embed/f2LucKU6QsM"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">    
                        <h3 class="mb-0">Task 2: Accelerometer</h3> <!------------------------------------------------>
                        <h4 class="mb-0">Accelerometer Equations</h4>
                        <p>The following equations for pitch (theta) and roll (phi) were found in the FastRobots-4-IMU 
                            Lecture Presentation. 
                        </p>
                        <img width="200" height="auto" src="Lab2/PitchRollEquations.png"></img>
                        <hr>
                        <h4 class="mb-0">Accelerometer Pitch and Roll at -90, 0, and 90 degrees</h4>
                        <p>These equations were used to compute pitch and roll for the accelerometer. I implemented the 
                            calculations in the Example1_Basics 
                            sketch (lines 247 - 255 below) that was used in the previous task to receive continuous data 
                            from the Artemis board and plot the results.
                        </p>
                        <img width="450" height="auto" src="Lab2/AddedExample1PitchRoll.png"></img>
                        <p>The below graphs show Pitch (data 0, green) and Roll (data 1, purple) at 0 degrees. The x-axis 
                            is time (sec) and the y-axis is degrees. 
                        </p>
                        <p>Pitch and Roll at 0 degrees:</p>
                        <img width="650" height="auto" src="Lab2/PitchAndRoll(0).png"></img>
                        <p>The below graphs show Pitch (data 0, green) at -90 and 90 degrees and the corresponding Roll 
                            (data 1, purple) outputs. The x-axis is time (sec) and the y-axis is degrees. From the graphs 
                            it can be seen that when pitch is at -90 degrees there is noise in the roll outputs.
                        </p>
                        <p>Pitch at -90 degrees:</p>
                        <img width="650" height="auto" src="Lab2/Pitch(-90).png"></img>
                        <p>Pitch at 90 degrees:</p>
                        <img width="650" height="auto" src="Lab2/Pitch(90).png"></img>
                        <p>The below graphs show Roll (data 1, purple) at -90 and 90 degrees and the corresponding Pitch 
                            (data 0, green) outputs. The x-axis is time (sec) and the y-axis is degrees. 
                        </p>
                        <p>Roll at -90 degrees:</p>
                        <img width="650" height="auto" src="Lab2/Roll(-90).png"></img>
                        <p>Roll at 90 degrees:</p>
                        <img width="650" height="auto" src="Lab2/Roll(90).png"></img>
                        <hr>
                        <h4 class="mb-0">Accuracy Analysis</h4>
                        <p>I plotted the data for pitch and roll at 0, 90, and -90 degrees in Jupyter Notebook and calculated 
                            the mean and standard deviation. 
                        </p>
                        <!-- <img width="650" height="auto" src="Lab2/AccelCode1.png"></img> -->
                        <!-- <img width="650" height="auto" src="Lab2/AccelCode2.png"></img> -->
                        <img width="650" height="auto" src="Lab2/AccelAnalysisPRPython.png"></img>
                        <p>Below are graphs. I also included a black line at 0, 90, or -90 degrees for reference. 
                            Note: The data for the graphs was collected by holding the IMU against the table using my hand. 
                            This may have resulted in some error due to the surface level or any small hand movements. In the
                            future the IMU should be calibrated when it is secured to the robot to achieve the most accurate 
                            pitch and roll data.
                        </p>
                        <img width="450" height="auto" src="Lab2/AccelPandR(0)Analysis.png"></img>
                        <img width="450" height="auto" src="Lab2/AccelAnalysisPitch(90).png"></img>
                        <img width="450" height="auto" src="Lab2/AccelAnalysisPitch(-90).png"></img>
                        <img width="450" height="auto" src="Lab2/AccelAnalysisRoll(90).png"></img>
                        <img width="450" height="auto" src="Lab2/AccelAnalysisRoll(-90).png"></img>
                        <hr>
                        <h4 class="mb-0">Fast Fourier Transform (FFT) Graphs</h4>
                        <p>The noise in the accelerometer was examined using a Fast Fourier Transform. Data for the FFT was 
                            collected as I moved the IMU. The sinusoid and the FFT graphs can be found below. There is a large 
                            spike closer to 0Hz and less noise as the frequency increases. 
                            Note: I modified the code from Stephan Wagner's Ed Discussion post to plot the FFT graphs.
                        </p>
                        <img width="450" height="auto" src="Lab2/PitchSin.png"></img>
                        <img width="450" height="auto" src="Lab2/PitchFFT.png"></img>
                        <img width="450" height="auto" src="Lab2/RollSin.png"></img>
                        <img width="450" height="auto" src="Lab2/RollFFT.png"></img> 
                        <hr>
                        <h4 class="mb-0">Low Pass Filter FFT Graphs</h4>
                        <p>From observing the above graphs, I choose a cutoff frequency of 25Hz to use for a low pass filter. 
                            Using the determined cutoff frequency, I calculated the alpha used in the low pass filter. The 
                            equations from lecture are below:
                        </p>
                        <img width="450" height="auto" src="Lab2/LPFalpha.png"></img>
                        <p>The low pass filter alpha was calculated to be 0.3698</p>
                        <p>The low pass filter was implemented in Arduino. The code to find pitch and roll for the accelerometer 
                            can be found below.
                        </p>
                        <img width="450" height="auto" src="Lab2/AccelArduino1.png"></img>
                        <img width="450" height="auto" src="Lab2/AccelArduino2.png"></img>
                        <p>The below FFT graphs show the original signal (red) and the reduction in noise after implementing a 
                            low pass filter (blue). Choosing a lower cut off frequency reduces more noise.
                        </p>
                        <img width="450" height="auto" src="Lab2/LPFPitchSin.png"></img>
                        <img width="450" height="auto" src="Lab2/LPFPitchFFT.png"></img>
                        <img width="450" height="auto" src="Lab2/LPFRollSin.png"></img>
                        <img width="450" height="auto" src="Lab2/LPFRollFFT.png"></img>
                        <p>Prior to implementing a low pass filter, there was not a significant level of noise in the IMU data 
                            due to a pre-existing internal low pass filter. The information for this can be found on page 10 of 
                            the <a href="https://www.mouser.com/datasheet/2/813/DS_000189_ICM_20948_v1_3-2489786.pdf">datasheet</a> 
                            for the IMU.
                        </p>
                        <img width="550" height="auto" src="Lab2/IMUDatasheet.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">    
                        <h3 class="mb-0">Task 3: Gyroscope</h3> <!--------------------------------------------------->
                        <h4 class="mb-0">Gyroscope Equations</h4>
                        <p>The following equations for pitch, roll, and yaw were found in the FastRobots-4-IMU Lecture Presentation.
                        </p>
                        <img width="450" height="auto" src="Lab2/GyroEquation.png"></img>
                        <p>The gyroscope measures angular velocity. To find pitch, roll, and yaw the data collected from the IMU 
                            needs to be multiplied by a time step (dt in the below code).
                        </p>
                        <img width="450" height="auto" src="Lab2/GyroExampleCode.png"></img>
                        <hr>
                        <h4 class="mb-0">Gyroscope vs. Accelerometer Outputs</h4>
                        <p>The code above was added to Example1_Basics and plotted alongside the data for the accelerometer's pitch 
                            and roll. The gyroscope data drifts over time, but has less noise then the accelerometer data. In the 
                            below graphs the IMU was moved slightly to achieve different outputs. 
                        </p>
                        <p>Pitch for Gyroscope (Blue) and Accelerometer (Orange):</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/3sH0axPc20A"></iframe>
                        <p>Roll for Gyroscope (Blue) and Accelerometer (Teal):</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/gdAvHs-LMqg"></iframe>  
                        <p>The drift for the gyroscope can be better seen in the below graphs. The gyroscope was held in the same 
                            position as the data was collected. As time increases the measurements drift away from the starting value.
                        </p>
                        <img width="450" height="auto" src="Lab2/GDriftPitch.png"></img> 
                        <img width="450" height="auto" src="Lab2/GDriftRoll.png"></img> 
                        <img width="450" height="auto" src="Lab2/GDriftYaw.png"></img> 
                        <p>Code to plot the above graphs: 
                        </p>
                        <img width="450" height="auto" src="Lab2/GyroArray.png"></img>
                        <hr>
                        <h4 class="mb-0">Adjusting The Sampling Frequency</h4>
                        <p>Decreasing the sampling frequency resulted in a slower update to the angle measurements and more variation 
                            in the outputs between each successive measurement. The sample rate should be high to achieve the greatest 
                            level of accuracy when recording data.
                        </p>
                        <hr>
                        <h4 class="mb-0">Complimentary Filter</h4>
                        <p>Complimentary filter equation from lecture:
                        </p>
                        <img width="450" height="auto" src="Lab2/GyroCompFilterEquation.png"></img> 
                        <img width="550" height="auto" src="Lab2/GyroCompExampleCode.png"></img>
                        <p>The complimentary filter shows resistance to small vibrations as the IMU is moved during data collection. The 
                            complimentary filter reduces noise compared to the accelerometer output. Alpha is a set constant that represents 
                            how well the accelerometer output measures the correct pitch and roll data. I examined how different alpha values 
                            affected the graphs. In the below graphs alpha = 0.5
                        </p>
                        <p>Pitch for Gyroscope (Blue), Gyroscope Complimentary Filter (Orange), and Accelerometer (Teal):</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/2p3gl1_OtOk"></iframe>
                        <p>Roll for Gyroscope (Blue), Gyroscope Complimentary Filter (Orange), and Accelerometer (Teal):</p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/lSK8tIzuze0"></iframe>
                        <p>Pitch for Gyroscope, Gyroscope Complimentary Filter, and Accelerometer (alpha = 0.5):</p>
                        <img width="450" height="auto" src="Lab2/GAPitch.png"></img> 
                        <img width="450" height="auto" src="Lab2/GAPitch2.png"></img> 
                        <hr>
                        <p>Roll for Gyroscope, Gyroscope Complimentary Filter, and Accelerometer (alpha = 0.5):</p>
                        <img width="450" height="auto" src="Lab2/GARoll.png"></img> 
                        <img width="450" height="auto" src="Lab2/GARoll2.png"></img> 
                        <hr>
                        <p>Additional graphs when alpha = 0.2</p>
                        <img width="450" height="auto" src="Lab2/GAPitchalpha.png"></img> 
                        <img width="450" height="auto" src="Lab2/GAPitchalpha2.png"></img> 
                        <img width="450" height="auto" src="Lab2/GARollalpha.png"></img> 
                        <img width="450" height="auto" src="Lab2/GARollalpha2.png"></img>
                        <p>Code to plot the above graphs: 
                        </p>
                        <img width="650" height="auto" src="Lab2/GyroCompArray.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">    
                        <h3 class="mb-0">Task 4: Sample Data</h3> <!------------------------------------------------>
                        <h4 class="mb-0">Speed Of Sampling</h4>
                        <p>All of the accelerometer and gyroscope data print statements and any delays in the ble_arduino.ino file were removed 
                            in order to speed up the execution of the main loop. A case called "ACC_GYR" was created that checked if the data was 
                            ready in the main loop, and if there was data available stored it in arrays. After speeding up the loop I found that 
                            new values were sampled every 0.04s, which is a sampling frequency of 1/0.04s = 25Hz. 
                        </p>
                        <hr>
                        <h4 class="mb-0">Collect And Store Time Stamped IMU Data In Arrays</h4>
                        <p>The data is stored in arrays on the Artemis board before being sent to Jupyter Notebook. There are 7 arrays including 
                            time (1); accelerometer: x (2), y (3), z (4); gyroscope: x (5), y (6), z (7). I choose to use separate arrays because
                            in the future I can decide what data I want to send to Jupyter Notebook. The data type used to store the data is a float.
                            I choose to use a float as it is less bytes then a double, allowing for more data to be stored on the memory of the 
                            Artemis. A float is 4 bytes, resulting in one packet of data information for the 7 arrays to be 28 bytes. The Artemis has 
                            384kB of RAM. Sampling at 0.04s (25Hz) will result in data being collected for 548 seconds. 
                        </p>
                        <p>Code to find X, Y, and Z for Accelerometer and Gyroscope.</p>
                            <img width="450" height="auto" src="Lab2/endcode1.png"></img>
                            <img width="550" height="auto" src="Lab2/endcode2.png"></img>
                        <p>Code to find pitch and roll for Accelerometer; and pitch, pitch compliment, roll, roll compliment, and yaw for Gyroscope.
                        </p>
                            <img width="550" height="auto" src="Lab2/Lab2NewPart1.png"></img>
                            <img width="550" height="auto" src="Lab2/Lab2NewPart2.png"></img>
                        </p>
                        <hr>
                        <h4 class="mb-0">At Least 5 Seconds of IMU Data Sent Over Bluetooth</h4>
                        <p> I collected data for the X,Y,Z positions of the accelerometer and gyroscope as well as the pitch and roll for the 
                            accelerometer; and pitch, roll, and yaw for the gyroscope.
                        </p>
                        <img width="550" height="auto" src="Lab2/SampleData.png"></img>
                        <img width="550" height="auto" src="Lab2/Lab2NewGraph.png"></img>
                        <img width="550" height="auto" src="Lab2/Lab2NewData.png"></img>
                        <!- <img width="550" height="auto" src="Lab2/SampleDataPrint.png"></img> 
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">    
                        <h3 class="mb-0">Task 5: Stunt Recording</h3> <!-------------------------------------------->
                        <p>The robot is moved using the controller. The robot can move very quickly, but it is hard to 
                            control the speed. If the robot changes directions too quickly there is a chance that it 
                            might flip over. It is also hard to slow down the robot to a stop. When slowing down the 
                            robot, the reverse needs to be applied. This is easy to over compensate and the robot might 
                            end up traveling in reverse rather than coming to a stop. 
                        </p>
                        <iframe width="550" height="400" src="https://youtube.com/embed/rJJAcrVZHNI"></iframe>
                        </div>
                    </div>
<!-- Lab 2 References ---------------------------------------------------------------------------------------------->
                <h2 class="mb-2.5">Lab 2 References</h2>
                <p>Thank you to all of the TAs that answered my questions. I referenced the past lab reports of 
                    Liam Kain, Rafael Gottlieb, Larry Lu, Julian Prieto, and Ignacio Romo.
                </p>
        </div>
    </section>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__________________________________________________________________________________________________________________>
<!__ Lab 1: The Artemis Board And Bluetooth ________________________________________________________________________>
                    <h2 class="mb-5">Lab 1: The Artemis Board and Bluetooth</h2>
<!-- Lab 1 Part 1 -------------------------------------------------------------------------------------------------->
                    <h2 class="mb-5">Part 1</h2>
                    <p>Test the connection between the Artemis board and the computer. Arduino IDE was installed and 
                        a number of tasks were completed to show the functionality of the Artemis board.
                    </p>
<!-- Lab 1 Part 1 Prelab ------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Prelab</h2>
                    <p>Arduino IDE 2.2.1 was installed onto my computer.</p>
                    <p>Materials include:
                        <li><a href="https://www.arduino.cc/en/software">Arduino IDE</a></li>
                        <li><a href="https://www.sparkfun.com/products/15443">SparkFun RedBoard Artemis Nano</a></li>
                        <li>USB-C to USB-C cable</li>
                    </p>
                    <p><a href="https://hackaday.io/project/181686-better-serial-plotter">Better Serial Plotter</a> 
                        was installed to plot the temperature in Task 4.
                    </p>
<!-- Lab 1 Part 1 Lab Tasks ---------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Tasks 1 and 2: "Example: Blink It Up"</h3>
                            <p>The instructions for 
                                <a href="https://learn.sparkfun.com/tutorials/artemis-development-with-arduino?_ga=2.30055167.1151850962.1594648676-1889762036.1574524297&_gac=1.19903818.1593457111.Cj0KCQjwoub3BRC6ARIsABGhnyahkG7hU2v-0bSiAeprvZ7c9v0XEKYdVHIIi_-J-m5YLdDBMc2P_goaAtA4EALw_wcB">Arduino Installation</a>
                                were followed to ensure proper connection between the Artemis board and the computer. 
                                The SparkFun Apollo3 Boards package was installed using the Boards Manager in Arduino 
                                IDE (shown below). 
                            </p>
                            <img width="450" height="auto" src="assets/img/BoardManagerLab1.png"></img>
                            <p>The first task involved a blinking LED on the Artemis board. Example code: 
                                File->Examples->01.Basics->Blink. A loop was implemented and used the function 
                                digitalWrite() to turn on the LED for one second and then turn off the LED for one 
                                second (shown below). 
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/RmXWC8GnBA8"></iframe>
                        </div>                        
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3: Example04_Serial</h3>
                            <p>In this example serial communication between the Artemis board and the computer was tested. 
                                Example code: File->Examples->Apollo3->Example04_Serial. 
                            </p>
                            <p>Below is an image and video of the Serial Monitor showing the outputs: "Hello", 
                                "This is the serial example", "Test 1", "Test 2", and "Test 3".
                            </p>
                            <img width="650" height="auto" src="assets/img/SerialExample.png"></img>
                            <iframe width="550" height="400" src="https://youtube.com/embed/SPOZG18yV_w"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 4: Example02_AnalogRead</h3>
                            <p>The temperature sensor of the Artemis board was tested. Example code:
                                File->Examples->Apollo3->Example02_AnalogRead. I covered the sensor with my hand to increase 
                                the temperature reading. At the beginning of the test the temperature is around 32.3 degrees 
                                Celsius and reaches a temperature of around 33.4 degrees Celsius (shown below).
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/mSfWqqGDKHo"></iframe>
                            <p>Another example shows various temperatures being plotted using Better Serial Plotter.
                            </p>
                            <img width="550" height="auto" src="assets/img/TempGraph.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 5: Example1_MicrophoneOutput</h3>
                            <p>The Microphone Output task tests the microphone of the Artemis board. Example code: 
                                File->Examples->PDM->Example1_MicrophoneOutput. During this task I spoke into the 
                                microphone to change the frequency (image and video shown below). 
                            </p>
                            <img width="550" height="auto" src="assets/img/MicrophoneOutputArduinoPicture.png"></img>
                            <iframe width="550" height="400" src="https://youtube.com/embed/4mBy6SBxSOk"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000 Level Task 1: LED and Musical A4</h3>
                            <p>The Artemis board was programmed to turn on the LED when the musical note A4 is played 
                                on a speaker (code below). I started with the Example01_MicrophoneOutput sketch, 
                                and I added if statements (lines 113-122 below). The note A4 is 440Hz 
                                <a href="https://ptolemy.berkeley.edu/eecs20/week8/scale.html#:~:text=The%20frequencies%20440Hz%20and%20880Hz,12%20notes%20in%20every%20octave.">(UC Berkeley Reference)</a>. 
                                The first if statement determines if the frequency is between 430Hz and 450Hz to account 
                                for any slight error in sound quality. If the frequency is within range an A4 statement 
                                will be displayed in the Serial Monitor and the LED on the Artemis board will turn on. 
                                The second if statement determines if the frequency is lower than 430Hz or greater than 
                                450Hz. If the frequency is lower than 430Hz or greater than 450Hz a Not A4 statement 
                                will be displayed in the Serial Monitor and the LED on the Artemis board will turn off.
                            </p>
                            <img width="550" height="auto" src="assets/img/LEDAndMusicalA4Code.png"></img>
                            <p>Image of the outputs in the serial monitor:
                            </p>
                            <img width="550" height="auto" src="assets/img/LEDAndMusicalA4Picture.png"></img>
                            <p>Video showing the LED turning on when the note A4 is played:
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/t5JuJGBvxds"></iframe>
                           <!-- <iframe width="550" height="400" src="https://youtube.com/embed/BfnU4FwXjzY"></iframe> -->
                            <p>I added to the LED and Musical A4 task by creating a musical tuner. There are eleven total 
                                if statements that determine the note being played (lines 113-168 below):
                                <li>A4       : 440Hz (range 430Hz - 450Hz)</li>
                                <li>B4 Flat  : 466Hz (range 456Hz - 476Hz)</li>
                                <li>B4       : 494Hz (range 484Hz - 504Hz)</li>
                                <li>C5       : 523Hz (range 513Hz - 533Hz)</li> 
                                <li>C5 Sharp : 554Hz (range 544Hz - 564Hz)</li> 
                                <li>D5       : 587Hz (range 577Hz - 597Hz)</li>
                                <li>D5 Sharp : 622Hz (range 612Hz - 632Hz)</li>
                                <li>E5       : 659Hz (range 649Hz - 669Hz)</li>
                                <li>F5       : 698Hz (range 688Hz - 708Hz)</li>
                                <li>F5 Sharp : 740Hz (range 730Hz - 750Hz)</li> 
                                <li>G5       : 784Hz (range 774Hz - 794Hz)</li>
                            </p>
                            <img width="550" height="auto" src="assets/img/MusicalTunerPart1.png"></img>
                            <img width="550" height="auto" src="assets/img/MusicalTunerPart2.png"></img>
                            <p>Video of various notes being played: 
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/FZQ0NdW-NL0"></iframe>
                            <p>Videos:
                                <a href="https://www.youtube.com/watch?v=buimPG01gcs">A4</a>
                                <a href="https://www.youtube.com/watch?v=QE8AMN7jsg0">B4</a>
                                <a href="https://www.youtube.com/watch?v=QJ8LUGsukzY">C5</a>
                                <a href="https://www.youtube.com/watch?v=qP9inYU8nQk">D5</a>
                                <a href="https://www.youtube.com/watch?v=Df9yjr0rBr0">E5</a>
                                <a href="https://www.youtube.com/watch?v=tGNBLJTdZvs">F5</a>
                                <a href="https://www.youtube.com/watch?v=1qx0tCQet_U">G5</a>
                            </p>
                        </div>
                    </div>
<!-- Lab 1 Part 1 Discussion --------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Discussion: Part 1</h2>
                    <p>I tested the connection between the Artemis board and the computer. Tasks performed included 
                        turning on and off an Artemis board LED, printing serial monitor outputs, viewing temperature 
                        data in the serial monitor, testing the microphone, and creating a musical tuner. 
                    </p>
<!-- Lab 1 Part 2 -------------------------------------------------------------------------------------------------->
                    <h2 class="mb-5">Part 2</h2>
                    <p>Tested the Bluetooth communication between the computer and the Artemis board. Python in a 
                        Jupyter Notebook was used for the computer and Arduino IDE was used for the Artemis board.
                    </p>
<!-- Lab 1 Part 2 Prelab: Setup ------------------------------------------------------------------------------------>
                    <h2 class="mb-2.5">Prelab: Setup</h2>
                    <p>During the setup for the lab I already had installed Python 3. I needed to install pip by 
                        running the line "python3 -m pip install --user virtualenv" in the Command Line Interface 
                        (CLI). I created a folder called MAE 5190 Lab where I put all of the Jupyter Notebook files. 
                        I then created a virtual environment by typing "python3 -m venv FastRobots_ble" in the CLI. 
                    </p>
                    <p>The virtual environment was activated by typing "source FastRobots_ble/bin/activate" and then 
                        deactivated by typing "deactivate". 
                    </p>
                    <p>The python packages were installed by entering "pip install numpy pyyaml colorama nest_asyncio 
                        bleak jupyterlab" into the CLI.
                    </p>
                    <p>CLI entered commands:
                    </p>
                    <img width="auto" height="50" src="assets/img/Lab1PrelabPart1.png"></img>
                    <img width="auto" height="75" src="assets/img/Lab1PrelabPart2.png"></img>
                    <p>I also needed to install matplotlib to use for the 5000 level tasks. I asked a TA for help with 
                        the installation. Below is a picture of the command entered into the CLI.
                    </p>
                    <img width="auto" height="30" src="assets/img/matplotlib.png"></img>
                    <p>To import modules into the Jupyter Notebook the following lines of code were run.
                    </p>
                    <img width="550" height="auto" src="assets/img/ImportModules.png"></img>
                    <p>After uploading the provided ble_ardunio.ino file to the Artemis board the MAC address was 
                        printed in the serial monitor. The MAC address is c0:83:34:6a:b2:3c The image below shows 
                        the Arduino serial monitor with the printed MAC address.
                    </p>
                    <img width="600" height="auto" src="assets/img/Lab1PrelabPart3.png"></img>              
<!-- Lab 1 Part 2 Prelab: Codebase --------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Prelab: Codebase</h2>
                    <p>I installed the provided codebase into the project directory and copied the provided 
                        ble_python directory into the project directory. Then the Jupyter Notebook was opened by 
                        entering jupyter lab into the CLI (shown below).
                    </p>
                    <img width="auto" height="20" src="assets/img/jupyterlab.png"></img>
                    <p>To set up Bluetooth communication, the MAC address needed to be changed in the Jupyter 
                        Notebook connection.yaml file to match the advertised MAC address from the Artemis board. 
                        Then unique UUID addresses needed to be used in the connection.yaml file and Arduino IDE 
                        because some devices in the lab may have the same MAC address. Using UUIDs ensures that 
                        the computer is receiving data from the correct Artemis board. The UUIDs were generated in 
                        the Jupyter Notebook demo.ipynb file and then entered into the connection.yaml and 
                        ble_arduino.ino files. Below is a picture of the lines of code used in the demo.ipynb file 
                        to generate the UUIDs.
                    </p>
                    <img width="500" height="auto" src="assets/img/UUIDsJupyter.png"></img>
                    <p>The UUIDs were entered into the ble_arduino.ino file:</p>
                    <img width="500" height="auto" src="assets/img/UUIDsArduino.png"></img>
                    <p>The UUIDs were entered into the connection.yaml file:</p>
                    <img width="500" height="auto" src="assets/img/UUIDsPython.png"></img>
                    <p>To connect to the Artemis the following lines were run in the Jupyter Notebook.</p>
                    <img width="650" height="auto" src="assets/img/ConnectionPython.png"></img>
                    <p>Several command types needed to be defined to complete the tasks outlined in the next section. 
                        Added commands include ECHO, GET_TIME_MILLIS, GET_TIME_MILLIS_LOOP, SEND_TIME_DATA, 
                        and GET_TEMP_READINGS.
                    </p>
                    <p>Command types defined in Arduino IDE:</p>
                    <img width="250" height="auto" src="assets/img/CommandTypesArduino.png"></img>
                    <p>Command types defined in Jupyter Notebook:</p>
                    <img width="250" height="auto" src="assets/img/CmdTypesPython.png"></img>
<!-- Lab 1 Part 2 Lab Tasks ---------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab Tasks</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 1: ECHO Command</h3>
                            <p>The first task was to send an ECHO command of a string value to the Artemis board and 
                                receive an augmented string on the host computer. This task tested the communication 
                                between the Artemis board and the computer. 
                            </p>
                            <p>The image below shows the Arduino IDE code.
                            </p>
                            <img width="550" height="auto" src="assets/img/EchoArduino.png"></img>
                            <p>Jupyter Notebook code to send the ECHO command and receive the augmented string from 
                                the Artemis board:
                            </p>
                            <img width="650" height="auto" src="assets/img/EchoPython.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 2: GET_TIME_MILLIS</h3>
                            <p>The next task adds another command to the Jupyter Notebook for the Artemis board to 
                                reply with a string of the current time. 
                            </p>
                            <p>The image below shows the Arduino IDE code.
                            </p>
                            <img width="500" height="auto" src="assets/img/GetTimeMillisArduino.png"></img>
                            <p>Jupyter Notebook code to send the GET_TIME_MILLIS command and receive the string 
                                with the current time:
                            </p>
                            <img width="650" height="auto" src="assets/img/GetTimeMillisPython.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 3: Notification Handler</h3>
                            <p>A notification handler was created in the Python Jupyter Notebook to receive a 
                                string value from the Artemis board. The GET_TIME_MILLIS command was used to 
                                get the time after starting the notification handler.
                            </p>
                            <p>The image below is the notification handler and the GET_TIME_MILLIS command in 
                                the Jupyter Notebook.
                            </p>
                            <img width="650" height="auto" src="assets/img/NotificationHandlerPython.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 4: Current Time Loop</h3>
                            <p>A loop was implemented to  get the current time in milliseconds from the Artemis 
                                board and send the information to the computer. The time information is received 
                                and processed by the notification handler. 
                            </p>
                            <p>To find the data transfer rate the length of the data was computed by assembling the 
                                individual outputs into an array in the Jupyter Notebook. The number of strings in 
                                each element of the array is 9, and the number of elements in the array is 748. The 
                                time difference of the final (40.428s) and starting time (35.434s) was found to be 
                                4.994 seconds. The data transfer rate is 
                                (748*9)bytes/4.994seconds = 1348.018 bytes/second
                            </p>
                            <p>The image below shows the Arduino IDE code for case GET_TIME_MILLIS_LOOP.
                            </p>
                            <img width="500" height="auto" src="assets/img/GetTimeMillisLoopArduino.png"></img>
                            <p>The image below shows the Jupyter Notebook command and some of the time outputs.
                            </p>
                            <img width="650" height="auto" src="assets/img/GetTimeMillisLoopPython.png"></img>
                            <p>The following statements in Jupyter Notebook printed the timeSarray and the length 
                                of the array that was used to compute the data transfer rate.
                            </p>
                            <img width="650" height="auto" src="assets/img/PrintStatements.png"></img>
                            <p>The video below shows all of the time outputs.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/PBhUSLNY_qY"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 5: SEND_TIME_DATA</h3>
                            <p>A globally defined array to store time stamps was implemented. Inside of the for loop 
                                each time stamp is put inside of the timearray. The data is then sent as an array 
                                to the computer using Jupyter Notebook. 
                            </p>
                            <p>To find the data transfer rate the length of the data was computed by assembling the 
                                individual outputs into an array in the Jupyter Notebook. The number of strings in 
                                each element of the array is 9, and the number of elements in the array is 2748. 
                                The time difference of the final (49.730s) and starting time (49.685s) was found to 
                                be 0.045 seconds. The data transfer rate is 
                                (2748*9)bytes/0.045seconds = 549600 bytes/second
                            </p>
                            <p>The image below shows the Arduino IDE code.
                            </p>
                            <img width="500" height="auto" src="assets/img/SendTimeDataArduino.png"></img>
                            <p>The image below shows the Jupyter Notebook command and some of the time outputs
                                in the array.
                            </p>
                            <img width="550" height="auto" src="assets/img/SendTimeDataPython.png"></img>
                            <p>The following statements in Jupyter Notebook printed the timeSarray and the length 
                                of the array that was used to compute the data transfer rate.
                            </p>
                            <img width="650" height="auto" src="assets/img/PrintStatements.png"></img>
                            <p>The video below shows all of the times in the array.
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/93hRV7vIr8Y"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 6: GET_TEMP_READINGS</h3>
                            <p>A second array was added to store temperature readings from the Artemis board. The 
                                temperature array is the same size as the time array. The time and temperature 
                                readings were taken at the same time. The data is sent as an array to the computer 
                                using Jupyter Notebook. 
                            </p>
                            <p>The image below shows the Arduino IDE code. 
                            </p>
                            <img width="550" height="auto" src="assets/img/GetTempReadingsArduino.png"></img>
                            <p>The image below shows the Jupyter Notebook command and some of the time and temperature
                                outputs in the array.
                            </p>
                            <img width="650" height="auto" src="assets/img/GetTempReadingsPython.png"></img>
                            <p>The video below shows all of the times and temperatures in the array. 
                            </p>
                            <iframe width="550" height="400" src="https://youtube.com/embed/ATMvf5GT68A"></iframe>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Task 7: Discussion For Task 4 and Task 5</h3>
                            <p>The method used in Task 4 involved sending the current time from the Artemis board to 
                                the computer. Each time was sent as an individual piece of data. The method used in 
                                Task 5 involved creating an array of times that was stored on the memory of the 
                                Artemis board and then sent to the computer to display the collected data. 
                            </p>
                            <p>The data transfer rate of the array method used in Task 5 (549600 bytes/second) is 
                                faster than the data transfer rate when sending individual pieces of data in Task 4 
                                (1348.018 bytes/second).
                            </p>
                            <p>An advantage of sending data individually as in Task 4 is that less memory on the 
                                Artemis board is being used, but the disadvantage is a slower data transfer rate. 
                                An advantage of sending data as an array as in Task 5 is that there is a faster data 
                                transfer rate, but a disadvantage is that more memory is being used on the Artemis 
                                to store the array.
                            </p>
                            <p>The Artemis board has a maximum storage of 384 kB of RAM. If 16 bit values at 150 Hz 
                                were sampled every 5 seconds, 256 values would be stored before running out of memory 
                                on the Artemis board. The calculations can be found below.
                            </p>
                            <img width="500" height="auto" src="assets/img/math.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000 Level Task 1: Effective Data Rate And Overhead</h3>
                            <p>I sent a message from the computer and received a reply from the Artemis board. The data 
                                rate for 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, and 120 bytes was tested. The 
                                data rate for 5 bytes is 132.722 bytes/s and the data rate for 120 bytes is 2003.210 
                                bytes/s. The plotted graph shows that as the number of bytes sent to the Artemis board 
                                increases, the data rate increases. Below are pictures of the Jupyter Notebook code, the 
                                graph, and the printed statements.
                            </p>
                            <img width="550" height="auto" src="assets/img/DataTranferNHPython.png"></img>
                            <img width="550" height="auto" src="assets/img/DataTransferPython.png"></img>
                            <img width="550" height="auto" src="assets/img/DataTransferGraph.png"></img>
                            <img width="650" height="auto" src="assets/img/DTPart1.png"></img>
                            <img width="650" height="auto" src="assets/img/DTPart2.png"></img>
                            <img width="650" height="auto" src="assets/img/DTPart3.png"></img>
                            <p>Additional graphs:</p>
                            <img width="550" height="auto" src="assets/img/DTGraph2.png"></img>
                            <img width="550" height="auto" src="assets/img/DTGraph3.png"></img>
                        </div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">5000 Level Task 2: Reliability</h3>
                            <p>I tested the reliability of sending data at a higher rate from the robot to the 
                                computer. To determine the reliability of the data I ran the GET_TEMP_READINGS 
                                command and compared the outputs printed in the Jupyter Notebook and the Arduino 
                                IDE Serial Monitor. 
                            </p>
                            <img width="650" height="auto" src="assets/img/ReliabilityArduino.png"></img>
                            <p>Outputs in the Jupyter Notebook (left) and the Arduino Serial Monitor (right):
                            </p>
                            <img width="650" height="auto" src="assets/img/ReliabilityArduinoAndPython.png"></img>
                            <p>After looking at the outputs, the time and temperature readings on the computer are 
                                the same as the Artemis board and no data is missed. To confirm I copied and pasted 
                                the data from each window into Matlab variables titled time1 and temp1 for the 
                                Jupyter Notebook data, and time2 and temp2 for the Artemis data. I wrote a script to 
                                compare the corresponding time and temperature data and record how many data points 
                                are the same and how many are different. If there is a data point in the Jupyter 
                                Notebook that does not correspond to the Artemis board, or vice versa, an error 
                                message is displayed. The output variables in the workspace show that all data points 
                                are the same. The reliability is 100%.
                            </p>
                            <img width="450" height="auto" src="assets/img/Lab1MatlabPart1.png"></img>
                            <img width="450" height="auto" src="assets/img/Lab1MatlabPart2.png"></img>
                        </div>
                    </div>
<!-- Lab 1 Part 2 Discussion --------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Discussion: Part 2</h2>
                    <p>I learned how to use Bluetooth communication and UUIDs, between the Artemis board and 
                        computer. I did not experience any significant problems connecting to Bluetooth. 
                    </p>
<!-- Lab 1 References ---------------------------------------------------------------------------------------------->
                    <h2 class="mb-2.5">Lab 1 References</h2>
                    <p>Thank you to all of the TAs that answered my questions. I referenced the past lab reports of 
                        Liam Kain, Rafael Gottlieb, Larry Lu, Julian Prieto, and Ignacio Romo.
                    </p>
                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
                                
        <!-- Scroll to Top Button-->
        <a class="scroll-to-top rounded" href="#page-top"><i class="fas fa-angle-up"></i></a>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
